{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ebf8e5a-2c67-4e4e-af29-7fcbb735efd2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Overview"
    }
   },
   "source": [
    "# Databricks Operational Workspace Setup Guide\n",
    "## Data Engineering Best Practices\n",
    "\n",
    "This notebook provides a comprehensive framework for establishing operational patterns in your Databricks workspace:\n",
    "\n",
    "* **Notebooks Structure** - Organized folder hierarchy and naming conventions\n",
    "* **Cluster Policies** - Standardized compute configurations and cost controls\n",
    "* **Secrets Management** - Secure credential access patterns\n",
    "* **Project-Based Catalogs** - Unity Catalog organization by project with medallion architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc5a6edb-8a5c-4da8-b492-d6a42d2b2ba2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "source": [
    "## 1. Notebooks Structure & Organization\n",
    "\n",
    "### Recommended Folder Hierarchy\n",
    "```\n",
    "/Workspace/\n",
    "├── Shared/\n",
    "│   ├── libraries/          # Reusable functions and utilities\n",
    "│   ├── configs/            # Configuration notebooks\n",
    "│   └── templates/          # Starter templates\n",
    "├── Projects/\n",
    "│   ├── CHAT/               # CHAT project (chat_catalog)\n",
    "│   │   ├── bronze/         # Raw data ingestion notebooks\n",
    "│   │   ├── silver/         # Data cleaning/transformation\n",
    "│   │   ├── gold/           # Business aggregates\n",
    "│   │   ├── orchestration/  # Workflow definitions\n",
    "│   │   └── tests/          # Unit and integration tests\n",
    "│   ├── SELMAN/             # SELMAN project (selman_catalog)\n",
    "│   │   ├── bronze/\n",
    "│   │   ├── silver/\n",
    "│   │   ├── gold/\n",
    "│   │   ├── orchestration/\n",
    "│   │   └── tests/\n",
    "└── Users/                  # Individual development workspaces\n",
    "```\n",
    "\n",
    "### Naming Conventions\n",
    "* **Notebooks:** `01_ingest_source_data.py`, `02_transform_customers.sql`\n",
    "* **Tables:** `{layer}_{domain}_{entity}` (e.g., `silver_clinical_patients`)\n",
    "* **Jobs:** `{project}_{layer}_{pipeline}` (e.g., `chat_bronze_daily_ingest`)\n",
    "* **Catalogs:** `{project_name}_catalog` (e.g., `chat_catalog`, `selman_catalog`)\n",
    "\n",
    "### Project-Catalog Alignment\n",
    "Each project folder corresponds to a Unity Catalog:\n",
    "* **CHAT project** → `chat_catalog` (bronze/silver/gold schemas)\n",
    "* **SELMAN project** → `selman_catalog` (bronze/silver/gold schemas)\n",
    "* Notebooks in project folders write to their respective catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard notebook header template for data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c669a3f2-a212-4123-b016-61a2539d9b62",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook Template Example"
    }
   },
   "outputs": [],
   "source": [
    "# Standard notebook header template for data engineering\n",
    "\n",
    "# ============================================\n",
    "# NOTEBOOK: Data Ingestion Template\n",
    "# PURPOSE: Ingest raw data from source systems\n",
    "# PROJECT: CHAT (or specify your project)\n",
    "# AUTHOR: Data Engineering Team\n",
    "# CREATED: 2026-01-27\n",
    "# ============================================\n",
    "\n",
    "# Import standard libraries\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Notebook parameters (for job orchestration)\n",
    "dbutils.widgets.text(\"project\", \"chat\", \"Project Name (chat/selman/etc)\")\n",
    "dbutils.widgets.text(\"layer\", \"bronze\", \"Target Layer (bronze/silver/gold)\")\n",
    "dbutils.widgets.text(\"run_date\", \"\", \"Run Date (YYYY-MM-DD)\")\n",
    "\n",
    "project = dbutils.widgets.get(\"project\")\n",
    "layer = dbutils.widgets.get(\"layer\")\n",
    "run_date = dbutils.widgets.get(\"run_date\") or datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Get project configuration\n",
    "config = WorkspaceConfig.get_project_config(project)\n",
    "\n",
    "print(f\"Project: {project}\")\n",
    "print(f\"Catalog: {config['catalog']}\")\n",
    "print(f\"Target Layer: {layer}\")\n",
    "print(f\"Run Date: {run_date}\")\n",
    "print(f\"Storage Path: {config['storage_path']}\")\n",
    "\n",
    "# Set current catalog\n",
    "spark.sql(f\"USE CATALOG {config['catalog']}\")\n",
    "spark.sql(f\"USE SCHEMA {layer}\")\n",
    "\n",
    "print(f\"\\nInitialized for {config['catalog']}.{layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a2403f-40e2-4429-bdc5-538b9bad4a93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration Management"
    }
   },
   "outputs": [],
   "source": [
    "# Centralized configuration pattern using Python dictionaries\n",
    "\n",
    "class WorkspaceConfig:\n",
    "    \"\"\"Centralized configuration for project-specific settings\"\"\"\n",
    "    \n",
    "    # Project-based catalog configuration\n",
    "    PROJECTS = {\n",
    "        \"chat\": {\n",
    "            \"catalog\": \"chat_catalog\",\n",
    "            \"storage_path\": \"/mnt/projects/chat/data\",\n",
    "            \"checkpoint_path\": \"/mnt/projects/chat/checkpoints\",\n",
    "            \"owner_team\": \"chat_engineering_team\",\n",
    "            \"description\": \"CHAT project data assets\"\n",
    "        },\n",
    "        \"selman\": {\n",
    "            \"catalog\": \"selman_catalog\",\n",
    "            \"storage_path\": \"/mnt/projects/selman/data\",\n",
    "            \"checkpoint_path\": \"/mnt/projects/selman/checkpoints\",\n",
    "            \"owner_team\": \"selman_engineering_team\",\n",
    "            \"description\": \"SELMAN project data assets\"\n",
    "        }\n",
    "        # Add more projects as needed\n",
    "    }\n",
    "    \n",
    "    # Standard schema layers (same across all projects)\n",
    "    LAYERS = {\n",
    "        \"bronze\": {\n",
    "            \"description\": \"Raw ingested datasets and tables\",\n",
    "            \"data_classification\": \"raw\"\n",
    "        },\n",
    "        \"silver\": {\n",
    "            \"description\": \"Cleaned and validated silver tables\",\n",
    "            \"data_classification\": \"cleaned\"\n",
    "        },\n",
    "        \"gold\": {\n",
    "            \"description\": \"Business-ready gold tables\",\n",
    "            \"data_classification\": \"curated\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_project_config(project: str) -> dict:\n",
    "        \"\"\"Get configuration for specified project\"\"\"\n",
    "        if project not in WorkspaceConfig.PROJECTS:\n",
    "            raise ValueError(f\"Invalid project: {project}. Must be one of {list(WorkspaceConfig.PROJECTS.keys())}\")\n",
    "        return WorkspaceConfig.PROJECTS[project]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer_config(layer: str) -> dict:\n",
    "        \"\"\"Get configuration for specified layer\"\"\"\n",
    "        if layer not in WorkspaceConfig.LAYERS:\n",
    "            raise ValueError(f\"Invalid layer: {layer}. Must be one of {list(WorkspaceConfig.LAYERS.keys())}\")\n",
    "        return WorkspaceConfig.LAYERS[layer]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_table_path(project: str, layer: str, table_name: str) -> str:\n",
    "        \"\"\"Generate fully qualified table path\"\"\"\n",
    "        project_config = WorkspaceConfig.get_project_config(project)\n",
    "        return f\"{project_config['catalog']}.{layer}.{table_name}\"\n",
    "\n",
    "# Usage example\n",
    "project = \"chat\"  # Change to your project name\n",
    "config = WorkspaceConfig.get_project_config(project)\n",
    "print(f\"Project: {project}\")\n",
    "print(f\"Catalog: {config['catalog']}\")\n",
    "print(f\"Storage Path: {config['storage_path']}\")\n",
    "print(f\"\\nTable Path Example: {WorkspaceConfig.get_table_path(project, 'silver', 'customers')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa83a12-3bba-42fc-ac3e-b51fcd51d94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Cluster Policies & Compute Management\n",
    "\n",
    "### Policy Strategy\n",
    "Cluster policies enforce standardized configurations and cost controls across environments.\n",
    "\n",
    "### Recommended Policies\n",
    "\n",
    "**Development Policy:**\n",
    "* Single-node or small clusters (1-3 workers)\n",
    "* Auto-termination: 30 minutes\n",
    "* Spot instances enabled\n",
    "* DBR: Latest LTS version\n",
    "\n",
    "**Production Policy:**\n",
    "* Fixed-size or autoscaling clusters\n",
    "* Auto-termination: 120 minutes\n",
    "* On-demand instances\n",
    "* DBR: Stable LTS version\n",
    "* Enhanced monitoring enabled\n",
    "\n",
    "**Interactive Policy:**\n",
    "* For ad-hoc analysis\n",
    "* Auto-termination: 60 minutes\n",
    "* Photon enabled for SQL workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development Cluster Policy (JSON format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4894c33a-7737-42f8-b4a1-5bc3125430ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dev Cluster Policy JSON"
    }
   },
   "outputs": [],
   "source": [
    "# Development Cluster Policy (JSON format)\n",
    "# Apply via Databricks Admin Console > Compute > Policies\n",
    "\n",
    "dev_cluster_policy = {\n",
    "    \"name\": \"Data Engineering - Development\",\n",
    "    \"definition\": {\n",
    "        \"spark_version\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"15.4.x-scala2.12\"\n",
    "        },\n",
    "        \"node_type_id\": {\n",
    "            \"type\": \"allowlist\",\n",
    "            \"values\": [\"n2-highmem-4\", \"n2-standard-4\"],\n",
    "            \"defaultValue\": \"n2-standard-4\"\n",
    "        },\n",
    "        \"num_workers\": {\n",
    "            \"type\": \"range\",\n",
    "            \"minValue\": 0,\n",
    "            \"maxValue\": 3,\n",
    "            \"defaultValue\": 1\n",
    "        },\n",
    "        \"autotermination_minutes\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": 30\n",
    "        },\n",
    "        \"gcp_attributes.use_preemptible_executors\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": True\n",
    "        },\n",
    "        \"data_security_mode\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"USER_ISOLATION\"\n",
    "        },\n",
    "        \"spark_conf.spark.databricks.cluster.profile\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"singleNode\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(dev_cluster_policy, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Production Cluster Policy (JSON format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5afab7-8ad4-4f83-848a-9b5ba00c63ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prod Cluster Policy JSON"
    }
   },
   "outputs": [],
   "source": [
    "# Production Cluster Policy (JSON format)\n",
    "\n",
    "prod_cluster_policy = {\n",
    "    \"name\": \"Data Engineering - Production\",\n",
    "    \"definition\": {\n",
    "        \"spark_version\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"15.4.x-scala2.12\"  # Use stable LTS\n",
    "        },\n",
    "        \"node_type_id\": {\n",
    "            \"type\": \"allowlist\",\n",
    "            \"values\": [\"n2-highmem-4\", \"n2-highmem-8\"],\n",
    "            \"defaultValue\": \"n2-highmem-4\"\n",
    "        },\n",
    "        \"autoscale\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": {\n",
    "                \"min_workers\": 2,\n",
    "                \"max_workers\": 10\n",
    "            }\n",
    "        },\n",
    "        \"autotermination_minutes\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": 120\n",
    "        },\n",
    "        \"gcp_attributes.use_preemptible_executors\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": False  # On-demand for production\n",
    "        },\n",
    "        \"data_security_mode\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"USER_ISOLATION\"\n",
    "        },\n",
    "        \"spark_conf.spark.databricks.delta.optimizeWrite.enabled\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"true\"\n",
    "        },\n",
    "        \"spark_conf.spark.databricks.delta.autoCompact.enabled\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"true\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(prod_cluster_policy, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07624320-8e61-46ce-a8fa-69431db2900f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Secrets Management & Secure Access\n",
    "\n",
    "### Databricks Secrets Architecture\n",
    "Use **Databricks Secrets** to securely store credentials, API keys, and connection strings.\n",
    "\n",
    "### Setup Steps\n",
    "\n",
    "**1. Create Secret Scopes** (via Databricks CLI or UI):\n",
    "```bash\n",
    "# Using Databricks CLI\n",
    "databricks secrets create-scope --scope dev-secrets\n",
    "databricks secrets create-scope --scope prod-secrets\n",
    "```\n",
    "\n",
    "**2. Add Secrets to Scopes:**\n",
    "```bash\n",
    "# Example: Store database password\n",
    "databricks secrets put --scope dev-secrets --key db-password\n",
    "\n",
    "# Example: Store API key\n",
    "databricks secrets put --scope prod-secrets --key api-key\n",
    "```\n",
    "\n",
    "**3. Grant Access via ACLs:**\n",
    "```bash\n",
    "# Grant read access to a group\n",
    "databricks secrets put-acl --scope prod-secrets --principal data-eng-team --permission READ\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "* Separate scopes per environment (dev/staging/prod)\n",
    "* Use GCP Secret Manager backend for production\n",
    "* Never hardcode credentials in notebooks\n",
    "* Rotate secrets regularly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secure secrets access pattern in notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4cc89a6-c8dd-452b-9cb3-66cd2e804e01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Secrets Access Pattern"
    }
   },
   "outputs": [],
   "source": [
    "# Secure secrets access pattern in notebooks\n",
    "\n",
    "class SecretsManager:\n",
    "    \"\"\"Wrapper for secure credential access\"\"\"\n",
    "    \n",
    "    def __init__(self, project: str):\n",
    "        self.project = project\n",
    "        self.scope = f\"{project}-secrets\"\n",
    "    \n",
    "    def get_secret(self, key: str) -> str:\n",
    "        \"\"\"Retrieve secret from Databricks secret scope\"\"\"\n",
    "        try:\n",
    "            return dbutils.secrets.get(scope=self.scope, key=key)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to retrieve secret '{key}' from scope '{self.scope}': {str(e)}\")\n",
    "    \n",
    "    def get_jdbc_connection(self, db_type: str) -> dict:\n",
    "        \"\"\"Get database connection parameters\"\"\"\n",
    "        return {\n",
    "            \"url\": self.get_secret(f\"{db_type}-url\"),\n",
    "            \"user\": self.get_secret(f\"{db_type}-user\"),\n",
    "            \"password\": self.get_secret(f\"{db_type}-password\")\n",
    "        }\n",
    "    \n",
    "    def get_api_credentials(self, service: str) -> dict:\n",
    "        \"\"\"Get API credentials for external services\"\"\"\n",
    "        return {\n",
    "            \"api_key\": self.get_secret(f\"{service}-api-key\"),\n",
    "            \"api_secret\": self.get_secret(f\"{service}-api-secret\")\n",
    "        }\n",
    "\n",
    "# Usage example for CHAT project\n",
    "project = \"chat\"  # Change to your project: 'chat', 'selman', etc.\n",
    "secrets = SecretsManager(project)\n",
    "\n",
    "# Access individual secrets\n",
    "# api_key = secrets.get_secret(\"external-api-key\")\n",
    "\n",
    "# Access database credentials\n",
    "# db_config = secrets.get_jdbc_connection(\"postgres\")\n",
    "\n",
    "print(f\"Project: {project}\")\n",
    "print(f\"Secrets scope: {secrets.scope}\")\n",
    "print(\"✓ Secrets manager initialized\")\n",
    "print(\"\\nNote: Create project-specific secret scopes:\")\n",
    "print(\"  databricks secrets create-scope --scope chat-secrets\")\n",
    "print(\"  databricks secrets create-scope --scope selman-secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Connecting to external database using secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679130d5-d902-4dca-b3e2-1708a78d8179",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Connection Example with Secrets"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Connecting to external database using secrets\n",
    "\n",
    "def read_from_external_db(table_name: str, project: str):\n",
    "    \"\"\"Read data from external database using secure credentials\"\"\"\n",
    "    \n",
    "    secrets = SecretsManager(project)\n",
    "    \n",
    "    # Retrieve connection details from project-specific secrets\n",
    "    jdbc_url = secrets.get_secret(\"postgres-url\")\n",
    "    jdbc_user = secrets.get_secret(\"postgres-user\")\n",
    "    jdbc_password = secrets.get_secret(\"postgres-password\")\n",
    "    \n",
    "    # Read data using JDBC\n",
    "    df = (spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", table_name)\n",
    "        .option(\"user\", jdbc_user)\n",
    "        .option(\"password\", jdbc_password)\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def write_to_project_catalog(df, project: str, layer: str, table_name: str):\n",
    "    \"\"\"Write DataFrame to project catalog\"\"\"\n",
    "    config = WorkspaceConfig.get_project_config(project)\n",
    "    table_path = f\"{config['catalog']}.{layer}.{table_name}\"\n",
    "    \n",
    "    print(f\"Writing to: {table_path}\")\n",
    "    \n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(table_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Data written to {table_path}\")\n",
    "    return table_path\n",
    "\n",
    "# Example usage (commented to avoid execution without secrets)\n",
    "# project = \"chat\"\n",
    "# df = read_from_external_db(\"public.customers\", project)\n",
    "# write_to_project_catalog(df, project, \"bronze\", \"raw_customers\")\n",
    "\n",
    "print(\"✓ Secure connection functions defined\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  1. Set up project-specific secrets\")\n",
    "print(\"  2. Read from external source\")\n",
    "print(\"  3. Write to project catalog (bronze/silver/gold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a656bde-e903-46d6-a2a4-213ef3da2485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Project-Based Catalog Strategy\n",
    "\n",
    "### Project Isolation Architecture\n",
    "\n",
    "**Catalog Organization:**\n",
    "\n",
    "Each project receives its own dedicated catalog in the Foundation Workspace with standardized medallion architecture:\n",
    "\n",
    "| Component | CHAT Project | SELMAN Project | Additional Projects |\n",
    "|-----------|--------------|----------------|---------------------|\n",
    "| **Catalog** | `chat_catalog` | `selman_catalog` | `{project}_catalog` |\n",
    "| **Storage** | `/mnt/projects/chat/` | `/mnt/projects/selman/` | `/mnt/projects/{project}/` |\n",
    "| **Secrets** | `chat-secrets` | `selman-secrets` | `{project}-secrets` |\n",
    "| **Owner Team** | CHAT Engineering | SELMAN Engineering | Project Team |\n",
    "| **Access** | Project-specific | Project-specific | Project-specific |\n",
    "\n",
    "### Standard Schema Structure (Per Project)\n",
    "\n",
    "Every project catalog contains three standard schemas:\n",
    "* **bronze** - Raw ingested datasets and tables from source systems\n",
    "* **silver** - Cleaned and validated silver tables\n",
    "* **gold** - Business-ready gold tables for analytics\n",
    "\n",
    "### Unity Catalog Setup\n",
    "```sql\n",
    "-- Create project-specific catalogs\n",
    "CREATE CATALOG IF NOT EXISTS chat_catalog\n",
    "  COMMENT 'CHAT project data assets - bronze, silver, and gold layers';\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS selman_catalog\n",
    "  COMMENT 'SELMAN project data assets - bronze, silver, and gold layers';\n",
    "\n",
    "-- Create schemas within CHAT catalog\n",
    "USE CATALOG chat_catalog;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze COMMENT 'Raw ingested datasets';\n",
    "CREATE SCHEMA IF NOT EXISTS silver COMMENT 'Cleaned and validated tables';\n",
    "CREATE SCHEMA IF NOT EXISTS gold COMMENT 'Business-ready analytics tables';\n",
    "\n",
    "-- Create schemas within SELMAN catalog\n",
    "USE CATALOG selman_catalog;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze COMMENT 'Raw ingested datasets';\n",
    "CREATE SCHEMA IF NOT EXISTS silver COMMENT 'Cleaned and validated tables';\n",
    "CREATE SCHEMA IF NOT EXISTS gold COMMENT 'Business-ready analytics tables';\n",
    "```\n",
    "\n",
    "### Benefits of Project-Based Catalogs\n",
    "* **Clear Ownership:** Each project team owns and manages their catalog\n",
    "* **Independent Access Control:** Project-specific permissions and security\n",
    "* **Simplified Cost Tracking:** Easy to track resource usage per project\n",
    "* **Flexible Lifecycle:** Projects can be archived or decommissioned independently\n",
    "* **Reduced Complexity:** No need for environment-based catalog multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project-aware data access pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4bf886a-be12-4eb2-abaf-14e589a7c460",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Project-Aware Table Access"
    }
   },
   "outputs": [],
   "source": [
    "# Project-aware data access pattern\n",
    "\n",
    "class DataAccessLayer:\n",
    "    \"\"\"Abstraction layer for project-aware data access\"\"\"\n",
    "    \n",
    "    def __init__(self, project: str):\n",
    "        self.config = WorkspaceConfig.get_project_config(project)\n",
    "        self.catalog = self.config['catalog']\n",
    "        self.project = project\n",
    "    \n",
    "    def get_table_path(self, layer: str, table_name: str) -> str:\n",
    "        \"\"\"Generate fully qualified table name\"\"\"\n",
    "        return f\"{self.catalog}.{layer}.{table_name}\"\n",
    "    \n",
    "    def read_table(self, layer: str, table_name: str):\n",
    "        \"\"\"Read table from specified layer\"\"\"\n",
    "        table_path = self.get_table_path(layer, table_name)\n",
    "        print(f\"Reading from: {table_path}\")\n",
    "        return spark.table(table_path)\n",
    "    \n",
    "    def write_table(self, df, layer: str, table_name: str, mode: str = \"overwrite\"):\n",
    "        \"\"\"Write DataFrame to specified layer\"\"\"\n",
    "        table_path = self.get_table_path(layer, table_name)\n",
    "        print(f\"Writing to: {table_path} (mode: {mode})\")\n",
    "        \n",
    "        (df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(mode)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .saveAsTable(table_path)\n",
    "        )\n",
    "        \n",
    "        return table_path\n",
    "    \n",
    "    def list_tables(self, layer: str):\n",
    "        \"\"\"List all tables in a specific layer\"\"\"\n",
    "        return spark.sql(f\"SHOW TABLES IN {self.catalog}.{layer}\")\n",
    "\n",
    "# Usage example for CHAT project\n",
    "project = \"chat\"  # Change to your project: 'chat', 'selman', etc.\n",
    "data_access = DataAccessLayer(project)\n",
    "\n",
    "# Example table paths\n",
    "print(f\"Project: {project}\")\n",
    "print(\"Bronze layer:\", data_access.get_table_path(\"bronze\", \"raw_events\"))\n",
    "print(\"Silver layer:\", data_access.get_table_path(\"silver\", \"cleaned_events\"))\n",
    "print(\"Gold layer:\", data_access.get_table_path(\"gold\", \"daily_metrics\"))\n",
    "\n",
    "# For working with multiple projects\n",
    "print(\"\\nMulti-project example:\")\n",
    "for proj in [\"chat\", \"selman\"]:\n",
    "    dal = DataAccessLayer(proj)\n",
    "    print(f\"{proj}: {dal.get_table_path('gold', 'summary_metrics')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project-based deployment and lifecycle management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c85d2c94-65fe-4e92-87f8-9efd605bad95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Project Deployment Pattern"
    }
   },
   "outputs": [],
   "source": [
    "# Project-based deployment and lifecycle management\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "class ProjectDeploymentManager:\n",
    "    \"\"\"Manage notebook deployments and project lifecycle\"\"\"\n",
    "    \n",
    "    def __init__(self, project: str, target_layer: str = None):\n",
    "        self.project = project\n",
    "        self.target_layer = target_layer\n",
    "        self.config = WorkspaceConfig.get_project_config(project)\n",
    "    \n",
    "    def validate_deployment(self) -> bool:\n",
    "        \"\"\"Pre-deployment validation checks\"\"\"\n",
    "        checks = {\n",
    "            \"project_exists\": self.project in WorkspaceConfig.PROJECTS,\n",
    "            \"catalog_accessible\": self._check_catalog_access(),\n",
    "            \"layer_valid\": self.target_layer in [\"bronze\", \"silver\", \"gold\", None]\n",
    "        }\n",
    "        \n",
    "        print(f\"Deployment Validation for Project: {self.project}\")\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"  {status} {check}: {result}\")\n",
    "        \n",
    "        return all(checks.values())\n",
    "    \n",
    "    def _check_catalog_access(self) -> bool:\n",
    "        \"\"\"Verify catalog accessibility\"\"\"\n",
    "        try:\n",
    "            catalog = self.config['catalog']\n",
    "            spark.sql(f\"USE CATALOG {catalog}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def deploy_to_layer(self, notebook_path: str, layer: str):\n",
    "        \"\"\"Deploy notebook for specific data layer\"\"\"\n",
    "        if not self.validate_deployment():\n",
    "            raise ValueError(\"Deployment validation failed\")\n",
    "        \n",
    "        print(f\"\\nDeploying {notebook_path}\")\n",
    "        print(f\"  Project: {self.project}\")\n",
    "        print(f\"  Catalog: {self.config['catalog']}\")\n",
    "        print(f\"  Target Layer: {layer}\")\n",
    "        print(\"\\n[Deployment would execute via Databricks CLI/API]\")\n",
    "    \n",
    "    def promote_data(self, table_name: str, source_layer: str, target_layer: str):\n",
    "        \"\"\"Promote data between layers within project\"\"\"\n",
    "        valid_promotions = [\n",
    "            (\"bronze\", \"silver\"),\n",
    "            (\"silver\", \"gold\")\n",
    "        ]\n",
    "        \n",
    "        if (source_layer, target_layer) not in valid_promotions:\n",
    "            raise ValueError(f\"Invalid promotion path: {source_layer} -> {target_layer}\")\n",
    "        \n",
    "        source_table = f\"{self.config['catalog']}.{source_layer}.{table_name}\"\n",
    "        target_table = f\"{self.config['catalog']}.{target_layer}.{table_name}\"\n",
    "        \n",
    "        print(f\"\\nData Promotion:\")\n",
    "        print(f\"  From: {source_table}\")\n",
    "        print(f\"  To: {target_table}\")\n",
    "        print(\"  [Quality gates and validation would run here]\")\n",
    "\n",
    "# Example usage for CHAT project\n",
    "project = \"chat\"  # Change to your project\n",
    "deployment = ProjectDeploymentManager(project=project)\n",
    "deployment.validate_deployment()\n",
    "\n",
    "# Example: Promote data from bronze to silver\n",
    "# deployment.promote_data(\"customer_data\", \"bronze\", \"silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit testing pattern for data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e30fd7e8-da08-4206-acb1-0a056276d9f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testing Framework"
    }
   },
   "outputs": [],
   "source": [
    "# Unit testing pattern for data pipelines\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "class DataQualityTests:\n",
    "    \"\"\"Data quality testing framework\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.results = []\n",
    "    \n",
    "    def test_not_empty(self, test_name: str = \"Not Empty\"):\n",
    "        \"\"\"Test that DataFrame is not empty\"\"\"\n",
    "        count = self.df.count()\n",
    "        passed = count > 0\n",
    "        self.results.append({\"test\": test_name, \"passed\": passed, \"details\": f\"Row count: {count}\"})\n",
    "        return self\n",
    "    \n",
    "    def test_no_nulls(self, columns: list, test_name: str = \"No Nulls\"):\n",
    "        \"\"\"Test that specified columns have no null values\"\"\"\n",
    "        for col in columns:\n",
    "            null_count = self.df.filter(F.col(col).isNull()).count()\n",
    "            passed = null_count == 0\n",
    "            self.results.append({\n",
    "                \"test\": f\"{test_name} - {col}\",\n",
    "                \"passed\": passed,\n",
    "                \"details\": f\"Null count: {null_count}\"\n",
    "            })\n",
    "        return self\n",
    "    \n",
    "    def test_unique(self, column: str, test_name: str = \"Unique Values\"):\n",
    "        \"\"\"Test that column has unique values\"\"\"\n",
    "        total_count = self.df.count()\n",
    "        distinct_count = self.df.select(column).distinct().count()\n",
    "        passed = total_count == distinct_count\n",
    "        self.results.append({\n",
    "            \"test\": f\"{test_name} - {column}\",\n",
    "            \"passed\": passed,\n",
    "            \"details\": f\"Total: {total_count}, Distinct: {distinct_count}\"\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Print test results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA QUALITY TEST RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        passed_count = sum(1 for r in self.results if r[\"passed\"])\n",
    "        total_count = len(self.results)\n",
    "        \n",
    "        for result in self.results:\n",
    "            status = \"✓ PASS\" if result[\"passed\"] else \"✗ FAIL\"\n",
    "            print(f\"{status} | {result['test']}: {result['details']}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"Results: {passed_count}/{total_count} tests passed\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return passed_count == total_count\n",
    "\n",
    "# Example usage with sample data\n",
    "sample_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"created_at\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "sample_data = [\n",
    "    (1, \"Record A\", datetime.now()),\n",
    "    (2, \"Record B\", datetime.now()),\n",
    "    (3, \"Record C\", datetime.now())\n",
    "]\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_data, sample_schema)\n",
    "\n",
    "# Run tests\n",
    "tests = DataQualityTests(sample_df)\n",
    "tests.test_not_empty().test_no_nulls([\"id\", \"name\"]).test_unique(\"id\").report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline monitoring and logging pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e656318e-ca44-47a5-98f2-a8943954c0cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Monitoring & Observability"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline monitoring and logging pattern\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Monitor pipeline execution and log metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str, environment: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.environment = environment\n",
    "        self.start_time = datetime.now()\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def log_metric(self, metric_name: str, value):\n",
    "        \"\"\"Log a pipeline metric\"\"\"\n",
    "        self.metrics[metric_name] = value\n",
    "        print(f\"[METRIC] {metric_name}: {value}\")\n",
    "    \n",
    "    def log_stage(self, stage_name: str, status: str = \"started\"):\n",
    "        \"\"\"Log pipeline stage\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{timestamp}] [{self.environment.upper()}] {stage_name}: {status}\")\n",
    "    \n",
    "    def finalize(self, status: str = \"success\"):\n",
    "        \"\"\"Finalize monitoring and log summary\"\"\"\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"PIPELINE EXECUTION SUMMARY: {self.pipeline_name}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Environment: {self.environment}\")\n",
    "        print(f\"Status: {status.upper()}\")\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        print(f\"Start Time: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if self.metrics:\n",
    "            print(\"\\nMetrics:\")\n",
    "            for metric, value in self.metrics.items():\n",
    "                print(f\"  - {metric}: {value}\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # In production, send metrics to monitoring system\n",
    "        # self._send_to_monitoring_system()\n",
    "    \n",
    "    def _send_to_monitoring_system(self):\n",
    "        \"\"\"Send metrics to external monitoring (e.g., Datadog, CloudWatch)\"\"\"\n",
    "        # Implementation would integrate with your monitoring platform\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "monitor = PipelineMonitor(\"customer_etl\", env)\n",
    "\n",
    "monitor.log_stage(\"Data Ingestion\", \"started\")\n",
    "monitor.log_metric(\"records_ingested\", 10000)\n",
    "monitor.log_stage(\"Data Ingestion\", \"completed\")\n",
    "\n",
    "monitor.log_stage(\"Data Transformation\", \"started\")\n",
    "monitor.log_metric(\"records_transformed\", 9850)\n",
    "monitor.log_stage(\"Data Transformation\", \"completed\")\n",
    "\n",
    "monitor.log_stage(\"Data Quality Checks\", \"started\")\n",
    "monitor.log_metric(\"quality_score\", 98.5)\n",
    "monitor.log_stage(\"Data Quality Checks\", \"completed\")\n",
    "\n",
    "monitor.finalize(status=\"success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Workspace Setup - Data Engineering Operation ...",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
