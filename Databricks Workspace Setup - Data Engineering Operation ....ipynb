{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ebf8e5a-2c67-4e4e-af29-7fcbb735efd2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Overview"
    }
   },
   "source": [
    "# Databricks Operational Workspace Setup Guide\n",
    "## Data Engineering Best Practices\n",
    "\n",
    "This notebook provides a comprehensive framework for establishing operational patterns in your Databricks workspace:\n",
    "\n",
    "* **Notebooks Structure** - Organized folder hierarchy and naming conventions\n",
    "* **Cluster Policies** - Standardized compute configurations and cost controls\n",
    "* **Secrets Management** - Secure credential access patterns\n",
    "* **Environment Separation** - Dev/Staging/Production isolation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc5a6edb-8a5c-4da8-b492-d6a42d2b2ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Notebooks Structure & Organization\n",
    "\n",
    "### Recommended Folder Hierarchy\n",
    "```\n",
    "/Workspace/\n",
    "├── Shared/\n",
    "│   ├── libraries/          # Reusable functions and utilities\n",
    "│   ├── configs/            # Configuration notebooks\n",
    "│   └── templates/          # Starter templates\n",
    "├── Projects/\n",
    "│   ├── project_name/\n",
    "│   │   ├── bronze/         # Raw data ingestion\n",
    "│   │   ├── silver/         # Cleaned/transformed data\n",
    "│   │   ├── gold/           # Business-level aggregates\n",
    "│   │   ├── orchestration/  # Workflow definitions\n",
    "│   │   └── tests/          # Unit and integration tests\n",
    "└── Users/                  # Individual development workspaces\n",
    "```\n",
    "\n",
    "### Naming Conventions\n",
    "* **Notebooks:** `01_ingest_source_data.py`, `02_transform_customers.sql`\n",
    "* **Tables:** `{env}_{layer}_{domain}_{entity}` (e.g., `prod_silver_sales_orders`)\n",
    "* **Jobs:** `{env}_{project}_{pipeline}` (e.g., `prod_sales_daily_etl`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c669a3f2-a212-4123-b016-61a2539d9b62",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook Template Example"
    }
   },
   "outputs": [],
   "source": [
    "# Standard notebook header template for data engineering\n",
    "\n",
    "# ============================================\n",
    "# NOTEBOOK: Data Ingestion Template\n",
    "# PURPOSE: Ingest raw data from source systems\n",
    "# AUTHOR: Data Engineering Team\n",
    "# ============================================\n",
    "\n",
    "# Import standard libraries\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Notebook parameters (for job orchestration)\n",
    "dbutils.widgets.text(\"environment\", \"dev\", \"Environment (dev/staging/prod)\")\n",
    "dbutils.widgets.text(\"run_date\", \"\", \"Run Date (YYYY-MM-DD)\")\n",
    "\n",
    "env = dbutils.widgets.get(\"environment\")\n",
    "run_date = dbutils.widgets.get(\"run_date\") or datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Environment: {env}\")\n",
    "print(f\"Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a2403f-40e2-4429-bdc5-538b9bad4a93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration Management"
    }
   },
   "outputs": [],
   "source": [
    "# Centralized configuration pattern using Python dictionaries\n",
    "\n",
    "class WorkspaceConfig:\n",
    "    \"\"\"Centralized configuration for environment-specific settings\"\"\"\n",
    "    \n",
    "    ENVIRONMENTS = {\n",
    "        \"dev\": {\n",
    "            \"catalog\": \"dev_catalog\",\n",
    "            \"schema\": \"engineering\",\n",
    "            \"storage_path\": \"/mnt/dev/data\",\n",
    "            \"checkpoint_path\": \"/mnt/dev/checkpoints\",\n",
    "            \"cluster_policy\": \"dev-policy-id\"\n",
    "        },\n",
    "        \"staging\": {\n",
    "            \"catalog\": \"staging_catalog\",\n",
    "            \"schema\": \"engineering\",\n",
    "            \"storage_path\": \"/mnt/staging/data\",\n",
    "            \"checkpoint_path\": \"/mnt/staging/checkpoints\",\n",
    "            \"cluster_policy\": \"staging-policy-id\"\n",
    "        },\n",
    "        \"prod\": {\n",
    "            \"catalog\": \"prod_catalog\",\n",
    "            \"schema\": \"engineering\",\n",
    "            \"storage_path\": \"/mnt/prod/data\",\n",
    "            \"checkpoint_path\": \"/mnt/prod/checkpoints\",\n",
    "            \"cluster_policy\": \"prod-policy-id\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_config(environment: str) -> dict:\n",
    "        \"\"\"Get configuration for specified environment\"\"\"\n",
    "        if environment not in WorkspaceConfig.ENVIRONMENTS:\n",
    "            raise ValueError(f\"Invalid environment: {environment}. Must be one of {list(WorkspaceConfig.ENVIRONMENTS.keys())}\")\n",
    "        return WorkspaceConfig.ENVIRONMENTS[environment]\n",
    "\n",
    "# Usage example\n",
    "config = WorkspaceConfig.get_config(env)\n",
    "print(f\"Catalog: {config['catalog']}\")\n",
    "print(f\"Storage Path: {config['storage_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa83a12-3bba-42fc-ac3e-b51fcd51d94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Cluster Policies & Compute Management\n",
    "\n",
    "### Policy Strategy\n",
    "Cluster policies enforce standardized configurations and cost controls across environments.\n",
    "\n",
    "### Recommended Policies\n",
    "\n",
    "**Development Policy:**\n",
    "* Single-node or small clusters (1-3 workers)\n",
    "* Auto-termination: 30 minutes\n",
    "* Spot instances enabled\n",
    "* DBR: Latest LTS version\n",
    "\n",
    "**Production Policy:**\n",
    "* Fixed-size or autoscaling clusters\n",
    "* Auto-termination: 120 minutes\n",
    "* On-demand instances\n",
    "* DBR: Stable LTS version\n",
    "* Enhanced monitoring enabled\n",
    "\n",
    "**Interactive Policy:**\n",
    "* For ad-hoc analysis\n",
    "* Auto-termination: 60 minutes\n",
    "* Photon enabled for SQL workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4894c33a-7737-42f8-b4a1-5bc3125430ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dev Cluster Policy JSON"
    }
   },
   "outputs": [],
   "source": [
    "# Development Cluster Policy (JSON format)\n",
    "# Apply via Databricks Admin Console > Compute > Policies\n",
    "\n",
    "dev_cluster_policy = {\n",
    "    \"name\": \"Data Engineering - Development\",\n",
    "    \"definition\": {\n",
    "        \"spark_version\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"15.4.x-scala2.12\"\n",
    "        },\n",
    "        \"node_type_id\": {\n",
    "            \"type\": \"allowlist\",\n",
    "            \"values\": [\"n2-highmem-4\", \"n2-standard-4\"],\n",
    "            \"defaultValue\": \"n2-standard-4\"\n",
    "        },\n",
    "        \"num_workers\": {\n",
    "            \"type\": \"range\",\n",
    "            \"minValue\": 0,\n",
    "            \"maxValue\": 3,\n",
    "            \"defaultValue\": 1\n",
    "        },\n",
    "        \"autotermination_minutes\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": 30\n",
    "        },\n",
    "        \"gcp_attributes.use_preemptible_executors\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": True\n",
    "        },\n",
    "        \"data_security_mode\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"USER_ISOLATION\"\n",
    "        },\n",
    "        \"spark_conf.spark.databricks.cluster.profile\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"singleNode\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(dev_cluster_policy, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5afab7-8ad4-4f83-848a-9b5ba00c63ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prod Cluster Policy JSON"
    }
   },
   "outputs": [],
   "source": [
    "# Production Cluster Policy (JSON format)\n",
    "\n",
    "prod_cluster_policy = {\n",
    "    \"name\": \"Data Engineering - Production\",\n",
    "    \"definition\": {\n",
    "        \"spark_version\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"15.4.x-scala2.12\"  # Use stable LTS\n",
    "        },\n",
    "        \"node_type_id\": {\n",
    "            \"type\": \"allowlist\",\n",
    "            \"values\": [\"n2-highmem-4\", \"n2-highmem-8\"],\n",
    "            \"defaultValue\": \"n2-highmem-4\"\n",
    "        },\n",
    "        \"autoscale\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": {\n",
    "                \"min_workers\": 2,\n",
    "                \"max_workers\": 10\n",
    "            }\n",
    "        },\n",
    "        \"autotermination_minutes\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": 120\n",
    "        },\n",
    "        \"gcp_attributes.use_preemptible_executors\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": False  # On-demand for production\n",
    "        },\n",
    "        \"data_security_mode\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"USER_ISOLATION\"\n",
    "        },\n",
    "        \"spark_conf.spark.databricks.delta.optimizeWrite.enabled\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"true\"\n",
    "        },\n",
    "        \"spark_conf.spark.databricks.delta.autoCompact.enabled\": {\n",
    "            \"type\": \"fixed\",\n",
    "            \"value\": \"true\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(prod_cluster_policy, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07624320-8e61-46ce-a8fa-69431db2900f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Secrets Management & Secure Access\n",
    "\n",
    "### Databricks Secrets Architecture\n",
    "Use **Databricks Secrets** to securely store credentials, API keys, and connection strings.\n",
    "\n",
    "### Setup Steps\n",
    "\n",
    "**1. Create Secret Scopes** (via Databricks CLI or UI):\n",
    "```bash\n",
    "# Using Databricks CLI\n",
    "databricks secrets create-scope --scope dev-secrets\n",
    "databricks secrets create-scope --scope prod-secrets\n",
    "```\n",
    "\n",
    "**2. Add Secrets to Scopes:**\n",
    "```bash\n",
    "# Example: Store database password\n",
    "databricks secrets put --scope dev-secrets --key db-password\n",
    "\n",
    "# Example: Store API key\n",
    "databricks secrets put --scope prod-secrets --key api-key\n",
    "```\n",
    "\n",
    "**3. Grant Access via ACLs:**\n",
    "```bash\n",
    "# Grant read access to a group\n",
    "databricks secrets put-acl --scope prod-secrets --principal data-eng-team --permission READ\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "* Separate scopes per environment (dev/staging/prod)\n",
    "* Use GCP Secret Manager backend for production\n",
    "* Never hardcode credentials in notebooks\n",
    "* Rotate secrets regularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4cc89a6-c8dd-452b-9cb3-66cd2e804e01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Secrets Access Pattern"
    }
   },
   "outputs": [],
   "source": [
    "# Secure secrets access pattern in notebooks\n",
    "\n",
    "class SecretsManager:\n",
    "    \"\"\"Wrapper for secure credential access\"\"\"\n",
    "    \n",
    "    def __init__(self, environment: str):\n",
    "        self.scope = f\"{environment}-secrets\"\n",
    "    \n",
    "    def get_secret(self, key: str) -> str:\n",
    "        \"\"\"Retrieve secret from Databricks secret scope\"\"\"\n",
    "        try:\n",
    "            return dbutils.secrets.get(scope=self.scope, key=key)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to retrieve secret '{key}' from scope '{self.scope}': {str(e)}\")\n",
    "    \n",
    "    def get_jdbc_connection(self, db_type: str) -> dict:\n",
    "        \"\"\"Get database connection parameters\"\"\"\n",
    "        return {\n",
    "            \"url\": self.get_secret(f\"{db_type}-url\"),\n",
    "            \"user\": self.get_secret(f\"{db_type}-user\"),\n",
    "            \"password\": self.get_secret(f\"{db_type}-password\")\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "secrets = SecretsManager(env)\n",
    "\n",
    "# Access individual secrets\n",
    "# api_key = secrets.get_secret(\"external-api-key\")\n",
    "\n",
    "# Access database credentials\n",
    "# db_config = secrets.get_jdbc_connection(\"postgres\")\n",
    "\n",
    "print(f\"Secrets scope: {secrets.scope}\")\n",
    "print(\"✓ Secrets manager initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679130d5-d902-4dca-b3e2-1708a78d8179",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Connection Example with Secrets"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Connecting to external database using secrets\n",
    "\n",
    "def read_from_external_db(table_name: str, environment: str):\n",
    "    \"\"\"Read data from external database using secure credentials\"\"\"\n",
    "    \n",
    "    secrets = SecretsManager(environment)\n",
    "    \n",
    "    # Retrieve connection details from secrets\n",
    "    jdbc_url = secrets.get_secret(\"postgres-url\")\n",
    "    jdbc_user = secrets.get_secret(\"postgres-user\")\n",
    "    jdbc_password = secrets.get_secret(\"postgres-password\")\n",
    "    \n",
    "    # Read data using JDBC\n",
    "    df = (spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", table_name)\n",
    "        .option(\"user\", jdbc_user)\n",
    "        .option(\"password\", jdbc_password)\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage (commented to avoid execution without secrets)\n",
    "# df = read_from_external_db(\"public.customers\", env)\n",
    "# display(df.limit(5))\n",
    "\n",
    "print(\"✓ Secure connection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a656bde-e903-46d6-a2a4-213ef3da2485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Environment Separation Strategy\n",
    "\n",
    "### Multi-Environment Architecture\n",
    "\n",
    "**Isolation Levels:**\n",
    "\n",
    "| Component | Dev | Staging | Production |\n",
    "|-----------|-----|---------|------------|\n",
    "| **Catalog** | `dev_catalog` | `staging_catalog` | `prod_catalog` |\n",
    "| **Storage** | `/mnt/dev/` | `/mnt/staging/` | `/mnt/prod/` |\n",
    "| **Secrets** | `dev-secrets` | `staging-secrets` | `prod-secrets` |\n",
    "| **Clusters** | Spot/Preemptible | Mixed | On-Demand |\n",
    "| **Access** | All engineers | Limited team | Restricted |\n",
    "\n",
    "### Promotion Strategy\n",
    "\n",
    "1. **Development:** Individual experimentation and feature development\n",
    "2. **Staging:** Integration testing with production-like data volumes\n",
    "3. **Production:** Automated deployments via CI/CD\n",
    "\n",
    "### Unity Catalog Setup\n",
    "```sql\n",
    "-- Create environment-specific catalogs\n",
    "CREATE CATALOG IF NOT EXISTS dev_catalog;\n",
    "CREATE CATALOG IF NOT EXISTS staging_catalog;\n",
    "CREATE CATALOG IF NOT EXISTS prod_catalog;\n",
    "\n",
    "-- Create schemas within catalogs\n",
    "CREATE SCHEMA IF NOT EXISTS dev_catalog.bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS dev_catalog.silver;\n",
    "CREATE SCHEMA IF NOT EXISTS dev_catalog.gold;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4bf886a-be12-4eb2-abaf-14e589a7c460",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Environment-Aware Table Access"
    }
   },
   "outputs": [],
   "source": [
    "# Environment-aware data access pattern\n",
    "\n",
    "class DataAccessLayer:\n",
    "    \"\"\"Abstraction layer for environment-aware data access\"\"\"\n",
    "    \n",
    "    def __init__(self, environment: str):\n",
    "        self.config = WorkspaceConfig.get_config(environment)\n",
    "        self.catalog = self.config['catalog']\n",
    "        self.schema = self.config['schema']\n",
    "    \n",
    "    def get_table_path(self, layer: str, table_name: str) -> str:\n",
    "        \"\"\"Generate fully qualified table name\"\"\"\n",
    "        return f\"{self.catalog}.{layer}.{table_name}\"\n",
    "    \n",
    "    def read_table(self, layer: str, table_name: str):\n",
    "        \"\"\"Read table from specified layer\"\"\"\n",
    "        table_path = self.get_table_path(layer, table_name)\n",
    "        print(f\"Reading from: {table_path}\")\n",
    "        return spark.table(table_path)\n",
    "    \n",
    "    def write_table(self, df, layer: str, table_name: str, mode: str = \"overwrite\"):\n",
    "        \"\"\"Write DataFrame to specified layer\"\"\"\n",
    "        table_path = self.get_table_path(layer, table_name)\n",
    "        print(f\"Writing to: {table_path} (mode: {mode})\")\n",
    "        \n",
    "        (df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(mode)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .saveAsTable(table_path)\n",
    "        )\n",
    "        \n",
    "        return table_path\n",
    "\n",
    "# Usage example\n",
    "data_access = DataAccessLayer(env)\n",
    "\n",
    "# Example table paths\n",
    "print(\"Bronze layer:\", data_access.get_table_path(\"bronze\", \"raw_events\"))\n",
    "print(\"Silver layer:\", data_access.get_table_path(\"silver\", \"cleaned_events\"))\n",
    "print(\"Gold layer:\", data_access.get_table_path(\"gold\", \"daily_metrics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c85d2c94-65fe-4e92-87f8-9efd605bad95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CI/CD Integration Pattern"
    }
   },
   "outputs": [],
   "source": [
    "# CI/CD deployment pattern for notebooks\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "class DeploymentManager:\n",
    "    \"\"\"Manage notebook deployments across environments\"\"\"\n",
    "    \n",
    "    def __init__(self, source_env: str, target_env: str):\n",
    "        self.source_env = source_env\n",
    "        self.target_env = target_env\n",
    "    \n",
    "    def validate_deployment(self) -> bool:\n",
    "        \"\"\"Pre-deployment validation checks\"\"\"\n",
    "        checks = {\n",
    "            \"source_env_valid\": self.source_env in [\"dev\", \"staging\"],\n",
    "            \"target_env_valid\": self.target_env in [\"staging\", \"prod\"],\n",
    "            \"promotion_path_valid\": self._validate_promotion_path()\n",
    "        }\n",
    "        \n",
    "        print(\"Deployment Validation:\")\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"  {status} {check}: {result}\")\n",
    "        \n",
    "        return all(checks.values())\n",
    "    \n",
    "    def _validate_promotion_path(self) -> bool:\n",
    "        \"\"\"Ensure proper promotion path (dev->staging->prod)\"\"\"\n",
    "        valid_paths = [\n",
    "            (\"dev\", \"staging\"),\n",
    "            (\"staging\", \"prod\")\n",
    "        ]\n",
    "        return (self.source_env, self.target_env) in valid_paths\n",
    "    \n",
    "    def deploy(self, notebook_path: str):\n",
    "        \"\"\"Deploy notebook to target environment\"\"\"\n",
    "        if not self.validate_deployment():\n",
    "            raise ValueError(\"Deployment validation failed\")\n",
    "        \n",
    "        print(f\"\\nDeploying {notebook_path}\")\n",
    "        print(f\"  From: {self.source_env}\")\n",
    "        print(f\"  To: {self.target_env}\")\n",
    "        print(\"\\n[Deployment would execute via Databricks CLI/API]\")\n",
    "\n",
    "# Example usage\n",
    "deployment = DeploymentManager(source_env=\"dev\", target_env=\"staging\")\n",
    "deployment.validate_deployment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e30fd7e8-da08-4206-acb1-0a056276d9f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testing Framework"
    }
   },
   "outputs": [],
   "source": [
    "# Unit testing pattern for data pipelines\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "class DataQualityTests:\n",
    "    \"\"\"Data quality testing framework\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.results = []\n",
    "    \n",
    "    def test_not_empty(self, test_name: str = \"Not Empty\"):\n",
    "        \"\"\"Test that DataFrame is not empty\"\"\"\n",
    "        count = self.df.count()\n",
    "        passed = count > 0\n",
    "        self.results.append({\"test\": test_name, \"passed\": passed, \"details\": f\"Row count: {count}\"})\n",
    "        return self\n",
    "    \n",
    "    def test_no_nulls(self, columns: list, test_name: str = \"No Nulls\"):\n",
    "        \"\"\"Test that specified columns have no null values\"\"\"\n",
    "        for col in columns:\n",
    "            null_count = self.df.filter(F.col(col).isNull()).count()\n",
    "            passed = null_count == 0\n",
    "            self.results.append({\n",
    "                \"test\": f\"{test_name} - {col}\",\n",
    "                \"passed\": passed,\n",
    "                \"details\": f\"Null count: {null_count}\"\n",
    "            })\n",
    "        return self\n",
    "    \n",
    "    def test_unique(self, column: str, test_name: str = \"Unique Values\"):\n",
    "        \"\"\"Test that column has unique values\"\"\"\n",
    "        total_count = self.df.count()\n",
    "        distinct_count = self.df.select(column).distinct().count()\n",
    "        passed = total_count == distinct_count\n",
    "        self.results.append({\n",
    "            \"test\": f\"{test_name} - {column}\",\n",
    "            \"passed\": passed,\n",
    "            \"details\": f\"Total: {total_count}, Distinct: {distinct_count}\"\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Print test results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA QUALITY TEST RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        passed_count = sum(1 for r in self.results if r[\"passed\"])\n",
    "        total_count = len(self.results)\n",
    "        \n",
    "        for result in self.results:\n",
    "            status = \"✓ PASS\" if result[\"passed\"] else \"✗ FAIL\"\n",
    "            print(f\"{status} | {result['test']}: {result['details']}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"Results: {passed_count}/{total_count} tests passed\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return passed_count == total_count\n",
    "\n",
    "# Example usage with sample data\n",
    "sample_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"created_at\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "sample_data = [\n",
    "    (1, \"Record A\", datetime.now()),\n",
    "    (2, \"Record B\", datetime.now()),\n",
    "    (3, \"Record C\", datetime.now())\n",
    "]\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_data, sample_schema)\n",
    "\n",
    "# Run tests\n",
    "tests = DataQualityTests(sample_df)\n",
    "tests.test_not_empty().test_no_nulls([\"id\", \"name\"]).test_unique(\"id\").report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e656318e-ca44-47a5-98f2-a8943954c0cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Monitoring & Observability"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline monitoring and logging pattern\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Monitor pipeline execution and log metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str, environment: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.environment = environment\n",
    "        self.start_time = datetime.now()\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def log_metric(self, metric_name: str, value):\n",
    "        \"\"\"Log a pipeline metric\"\"\"\n",
    "        self.metrics[metric_name] = value\n",
    "        print(f\"[METRIC] {metric_name}: {value}\")\n",
    "    \n",
    "    def log_stage(self, stage_name: str, status: str = \"started\"):\n",
    "        \"\"\"Log pipeline stage\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{timestamp}] [{self.environment.upper()}] {stage_name}: {status}\")\n",
    "    \n",
    "    def finalize(self, status: str = \"success\"):\n",
    "        \"\"\"Finalize monitoring and log summary\"\"\"\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"PIPELINE EXECUTION SUMMARY: {self.pipeline_name}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Environment: {self.environment}\")\n",
    "        print(f\"Status: {status.upper()}\")\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        print(f\"Start Time: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if self.metrics:\n",
    "            print(\"\\nMetrics:\")\n",
    "            for metric, value in self.metrics.items():\n",
    "                print(f\"  - {metric}: {value}\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # In production, send metrics to monitoring system\n",
    "        # self._send_to_monitoring_system()\n",
    "    \n",
    "    def _send_to_monitoring_system(self):\n",
    "        \"\"\"Send metrics to external monitoring (e.g., Datadog, CloudWatch)\"\"\"\n",
    "        # Implementation would integrate with your monitoring platform\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "monitor = PipelineMonitor(\"customer_etl\", env)\n",
    "\n",
    "monitor.log_stage(\"Data Ingestion\", \"started\")\n",
    "monitor.log_metric(\"records_ingested\", 10000)\n",
    "monitor.log_stage(\"Data Ingestion\", \"completed\")\n",
    "\n",
    "monitor.log_stage(\"Data Transformation\", \"started\")\n",
    "monitor.log_metric(\"records_transformed\", 9850)\n",
    "monitor.log_stage(\"Data Transformation\", \"completed\")\n",
    "\n",
    "monitor.log_stage(\"Data Quality Checks\", \"started\")\n",
    "monitor.log_metric(\"quality_score\", 98.5)\n",
    "monitor.log_stage(\"Data Quality Checks\", \"completed\")\n",
    "\n",
    "monitor.finalize(status=\"success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Workspace Setup - Data Engineering Operation ...",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
