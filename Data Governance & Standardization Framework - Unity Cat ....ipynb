{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e49e31a9-1b3f-4529-ac1e-43b10997adf7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Overview"
    }
   },
   "source": [
    "# Data Governance & Standardization Framework\n",
    "## Unity Catalog Implementation Guide\n",
    "\n",
    "This notebook establishes comprehensive data governance and standardization practices aligned with Databricks best practices using Unity Catalog.\n",
    "\n",
    "## Framework Components\n",
    "* **Catalog Structure** - Three-level namespace organization (catalog.schema.table)\n",
    "* **Access Control** - Role-based permissions and privilege management\n",
    "* **Schema Governance** - Enforced data types, constraints, and metadata standards\n",
    "* **Quality Gates** - Automated validation checkpoints for data quality assurance\n",
    "* **Audit & Compliance** - Lineage tracking and access monitoring\n",
    "* **Standards Enforcement** - Automated checks for priority datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bcbd6c1-bef1-47cb-813d-227d5747840a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Unity Catalog Architecture & Setup\n",
    "\n",
    "### Three-Level Namespace Hierarchy\n",
    "\n",
    "```\n",
    "Catalog (Top Level)\n",
    "  ├── Schema (Database)\n",
    "  │     ├── Tables\n",
    "  │     ├── Views\n",
    "  │     ├── Functions\n",
    "  │     └── Volumes (for files)\n",
    "  └── Schema\n",
    "        └── ...\n",
    "```\n",
    "\n",
    "### Recommended Catalog Structure\n",
    "\n",
    "**By Environment:**\n",
    "* `dev_catalog` - Development and experimentation\n",
    "* `staging_catalog` - Pre-production testing\n",
    "* `prod_catalog` - Production data assets\n",
    "\n",
    "**By Domain (Alternative):**\n",
    "* `clinical_catalog` - Clinical and patient data\n",
    "* `operational_catalog` - Operational and administrative data\n",
    "* `analytics_catalog` - Analytics and reporting datasets\n",
    "\n",
    "### Schema Organization (Medallion Architecture)\n",
    "* `bronze` - Raw, unprocessed data from source systems\n",
    "* `silver` - Cleaned, validated, and conformed data\n",
    "* `gold` - Business-level aggregates and analytics-ready datasets\n",
    "* `sandbox` - Experimental and ad-hoc analysis workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How to Create environment-specific catalogs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Create environment-specific catalogs\n",
    "CREATE CATALOG IF NOT EXISTS dev_catalog\n",
    "  COMMENT 'Development environment for data engineering and testing';\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS staging_catalog\n",
    "  COMMENT 'Staging environment for pre-production validation';\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS prod_catalog\n",
    "  COMMENT 'Production environment for business-critical data assets';\n",
    "\n",
    "-- Show all catalogs\n",
    "SHOW CATALOGS;\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How to create medallion architecture schemas in production catalog**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Create medallion architecture schemas in production catalog\n",
    "USE CATALOG prod_catalog;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS bronze\n",
    "  COMMENT 'Raw data layer - unprocessed data from source systems'\n",
    "  WITH DBPROPERTIES (\n",
    "    'layer' = 'bronze',\n",
    "    'data_classification' = 'raw',\n",
    "    'owner_team' = 'data_engineering'\n",
    "  );\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS silver\n",
    "  COMMENT 'Cleaned data layer - validated and conformed data'\n",
    "  WITH DBPROPERTIES (\n",
    "    'layer' = 'silver',\n",
    "    'data_classification' = 'cleaned',\n",
    "    'owner_team' = 'data_engineering'\n",
    "  );\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS gold\n",
    "  COMMENT 'Business data layer - aggregated and analytics-ready'\n",
    "  WITH DBPROPERTIES (\n",
    "    'layer' = 'gold',\n",
    "    'data_classification' = 'curated',\n",
    "    'owner_team' = 'analytics'\n",
    "  );\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS sandbox\n",
    "  COMMENT 'Experimental workspace for ad-hoc analysis'\n",
    "  WITH DBPROPERTIES (\n",
    "    'layer' = 'sandbox',\n",
    "    'data_classification' = 'experimental',\n",
    "    'owner_team' = 'all_users'\n",
    "  );\n",
    "\n",
    "-- Show all schemas\n",
    "SHOW SCHEMAS IN prod_catalog;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d07bde-4134-4424-8fc7-6fc728d9400f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Access Control & Permissions Management\n",
    "\n",
    "### Unity Catalog Privilege Model\n",
    "\n",
    "**Securable Objects:**\n",
    "* Catalog\n",
    "* Schema\n",
    "* Table/View\n",
    "* Volume\n",
    "* Function\n",
    "\n",
    "**Key Privileges:**\n",
    "\n",
    "| Privilege | Description | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| `USE CATALOG` | Access catalog | Required to see catalog contents |\n",
    "| `USE SCHEMA` | Access schema | Required to see schema contents |\n",
    "| `SELECT` | Read data | Analysts, reporting tools |\n",
    "| `MODIFY` | Insert/Update/Delete | ETL processes |\n",
    "| `CREATE TABLE` | Create tables | Data engineers |\n",
    "| `ALL PRIVILEGES` | Full control | Catalog owners |\n",
    "\n",
    "### Role-Based Access Control (RBAC) Strategy\n",
    "\n",
    "**Data Engineering Team:**\n",
    "* Full access to `dev_catalog`\n",
    "* CREATE, MODIFY, SELECT on `prod_catalog.bronze` and `prod_catalog.silver`\n",
    "* SELECT on `prod_catalog.gold`\n",
    "\n",
    "**Analytics Team:**\n",
    "* SELECT on all `prod_catalog` schemas\n",
    "* CREATE, MODIFY on `prod_catalog.sandbox`\n",
    "\n",
    "**Business Users:**\n",
    "* SELECT on `prod_catalog.gold` only\n",
    "* No access to bronze/silver layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How to Grant permissions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Grant permissions to Data Engineering group\n",
    "GRANT USE CATALOG ON CATALOG prod_catalog TO `data_engineering_team`;\n",
    "GRANT USE SCHEMA ON SCHEMA prod_catalog.bronze TO `data_engineering_team`;\n",
    "GRANT USE SCHEMA ON SCHEMA prod_catalog.silver TO `data_engineering_team`;\n",
    "GRANT USE SCHEMA ON SCHEMA prod_catalog.gold TO `data_engineering_team`;\n",
    "\n",
    "GRANT CREATE TABLE ON SCHEMA prod_catalog.bronze TO `data_engineering_team`;\n",
    "GRANT CREATE TABLE ON SCHEMA prod_catalog.silver TO `data_engineering_team`;\n",
    "GRANT SELECT, MODIFY ON SCHEMA prod_catalog.bronze TO `data_engineering_team`;\n",
    "GRANT SELECT, MODIFY ON SCHEMA prod_catalog.silver TO `data_engineering_team`;\n",
    "GRANT SELECT ON SCHEMA prod_catalog.gold TO `data_engineering_team`;\n",
    "\n",
    "-- Grant permissions to Analytics group\n",
    "GRANT USE CATALOG ON CATALOG prod_catalog TO `analytics_team`;\n",
    "GRANT USE SCHEMA ON SCHEMA prod_catalog.gold TO `analytics_team`;\n",
    "GRANT USE SCHEMA ON SCHEMA prod_catalog.sandbox TO `analytics_team`;\n",
    "GRANT SELECT ON SCHEMA prod_catalog.gold TO `analytics_team`;\n",
    "GRANT CREATE TABLE, SELECT, MODIFY ON SCHEMA prod_catalog.sandbox TO `analytics_team`;\n",
    "\n",
    "-- Grant permissions to Business Users group\n",
    "GRANT USE CATALOG ON CATALOG prod_catalog TO `business_users`;\n",
    "GRANT USE SCHEMA ON SCHEMA prod_catalog.gold TO `business_users`;\n",
    "GRANT SELECT ON SCHEMA prod_catalog.gold TO `business_users`;\n",
    "\n",
    "-- Show grants on catalog\n",
    "SHOW GRANTS ON CATALOG prod_catalog;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53ee1a9c-6b93-4e7c-a3d6-e52d9c330b1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permission Management Helper"
    }
   },
   "outputs": [],
   "source": [
    "# Permission management utility class\n",
    "\n",
    "class PermissionManager:\n",
    "    \"\"\"Manage Unity Catalog permissions programmatically\"\"\"\n",
    "    \n",
    "    def __init__(self, catalog: str):\n",
    "        self.catalog = catalog\n",
    "    \n",
    "    def grant_read_access(self, schema: str, principal: str):\n",
    "        \"\"\"Grant read-only access to a schema\"\"\"\n",
    "        grants = [\n",
    "            f\"GRANT USE CATALOG ON CATALOG {self.catalog} TO `{principal}`\",\n",
    "            f\"GRANT USE SCHEMA ON SCHEMA {self.catalog}.{schema} TO `{principal}`\",\n",
    "            f\"GRANT SELECT ON SCHEMA {self.catalog}.{schema} TO `{principal}`\"\n",
    "        ]\n",
    "        \n",
    "        for grant in grants:\n",
    "            print(f\"Executing: {grant}\")\n",
    "            try:\n",
    "                spark.sql(grant)\n",
    "                print(\"  ✓ Success\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {str(e)}\")\n",
    "    \n",
    "    def grant_write_access(self, schema: str, principal: str):\n",
    "        \"\"\"Grant write access to a schema\"\"\"\n",
    "        grants = [\n",
    "            f\"GRANT USE CATALOG ON CATALOG {self.catalog} TO `{principal}`\",\n",
    "            f\"GRANT USE SCHEMA ON SCHEMA {self.catalog}.{schema} TO `{principal}`\",\n",
    "            f\"GRANT CREATE TABLE ON SCHEMA {self.catalog}.{schema} TO `{principal}`\",\n",
    "            f\"GRANT SELECT, MODIFY ON SCHEMA {self.catalog}.{schema} TO `{principal}`\"\n",
    "        ]\n",
    "        \n",
    "        for grant in grants:\n",
    "            print(f\"Executing: {grant}\")\n",
    "            try:\n",
    "                spark.sql(grant)\n",
    "                print(\"  ✓ Success\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {str(e)}\")\n",
    "    \n",
    "    def revoke_access(self, schema: str, principal: str):\n",
    "        \"\"\"Revoke all access from a schema\"\"\"\n",
    "        revokes = [\n",
    "            f\"REVOKE ALL PRIVILEGES ON SCHEMA {self.catalog}.{schema} FROM `{principal}`\"\n",
    "        ]\n",
    "        \n",
    "        for revoke in revokes:\n",
    "            print(f\"Executing: {revoke}\")\n",
    "            try:\n",
    "                spark.sql(revoke)\n",
    "                print(\"  ✓ Success\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {str(e)}\")\n",
    "    \n",
    "    def audit_permissions(self, schema: str = None):\n",
    "        \"\"\"Audit current permissions\"\"\"\n",
    "        if schema:\n",
    "            query = f\"SHOW GRANTS ON SCHEMA {self.catalog}.{schema}\"\n",
    "        else:\n",
    "            query = f\"SHOW GRANTS ON CATALOG {self.catalog}\"\n",
    "        \n",
    "        print(f\"\\nPermission Audit: {query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        try:\n",
    "            df = spark.sql(query)\n",
    "            display(df)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "print(\"Permission Manager initialized\")\n",
    "print(\"Usage: pm = PermissionManager('prod_catalog')\")\n",
    "print(\"       pm.grant_read_access('gold', 'user@example.com')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Governance & Standardization\n",
    "\n",
    "### Table Naming Conventions\n",
    "\n",
    "**Format:** `{domain}_{entity}_{type}`\n",
    "\n",
    "**Examples:**\n",
    "* `patient_demographics_dim` - Dimension table for patient demographics\n",
    "* `clinical_encounters_fact` - Fact table for clinical encounters\n",
    "* `lab_results_raw` - Raw lab results data\n",
    "* `pharmacy_orders_agg` - Aggregated pharmacy orders\n",
    "\n",
    "### Required Table Properties\n",
    "\n",
    "All production tables must include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata template for datasets\n",
    "metadata_template = {\n",
    "    'owner': 'team_name',\n",
    "    'domain': 'clinical|operational|analytics',\n",
    "    'pii_flag': 'true|false',\n",
    "    'retention_days': '365|730|2555',  # 1yr, 2yr, 7yr\n",
    "    'data_classification': 'public|internal|confidential|restricted',\n",
    "    'quality_tier': 'bronze|silver|gold',\n",
    "    'update_frequency': 'realtime|hourly|daily|weekly|monthly',\n",
    "    'source_system': 'system_name',\n",
    "    'business_owner': 'owner_email'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6047386-3f20-4e42-9ca6-7ec06ee05150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Column Naming Standards\n",
    "\n",
    "* Use `snake_case` for all column names\n",
    "* Prefix foreign keys with table name: `patient_id`, `encounter_id`\n",
    "* Use standard suffixes:\n",
    "  * `_id` for identifiers\n",
    "  * `_date` for dates\n",
    "  * `_timestamp` for timestamps\n",
    "  * `_flag` for boolean indicators\n",
    "  * `_code` for coded values\n",
    "  * `_desc` for descriptions\n",
    "\n",
    "### Data Type Standards\n",
    "\n",
    "| Data Category | Standard Type | Example |\n",
    "|---------------|---------------|----------|\n",
    "| Identifiers | `BIGINT` or `STRING` | patient_id BIGINT |\n",
    "| Dates | `DATE` | admission_date DATE |\n",
    "| Timestamps | `TIMESTAMP` | created_timestamp TIMESTAMP |\n",
    "| Currency | `DECIMAL(18,2)` | amount DECIMAL(18,2) |\n",
    "| Percentages | `DECIMAL(5,2)` | rate DECIMAL(5,2) |\n",
    "| Flags | `BOOLEAN` | is_active BOOLEAN |\n",
    "| Codes | `STRING` | diagnosis_code STRING |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b4f498c-69b0-4afe-821e-c11262d1ac55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Table Creation Template"
    }
   },
   "outputs": [],
   "source": [
    "# Standardized table creation with governance metadata\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType, DecimalType, BooleanType\n",
    "from datetime import datetime\n",
    "\n",
    "class GovernedTableBuilder:\n",
    "    \"\"\"Build tables with enforced governance standards\"\"\"\n",
    "    \n",
    "    def __init__(self, catalog: str, schema: str):\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.table_properties = {}\n",
    "        self.required_properties = [\n",
    "            'owner', 'domain', 'pii_flag', 'retention_days',\n",
    "            'data_classification', 'quality_tier', 'update_frequency',\n",
    "            'source_system', 'business_owner'\n",
    "        ]\n",
    "    \n",
    "    def set_properties(self, **kwargs):\n",
    "        \"\"\"Set table properties with validation\"\"\"\n",
    "        self.table_properties.update(kwargs)\n",
    "        return self\n",
    "    \n",
    "    def validate_properties(self) -> bool:\n",
    "        \"\"\"Validate required properties are present\"\"\"\n",
    "        missing = [prop for prop in self.required_properties if prop not in self.table_properties]\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"✗ Missing required properties: {', '.join(missing)}\")\n",
    "            return False\n",
    "        \n",
    "        print(\"✓ All required properties present\")\n",
    "        return True\n",
    "    \n",
    "    def create_table(self, table_name: str, schema: StructType, df=None, mode: str = \"overwrite\"):\n",
    "        \"\"\"Create table with governance metadata\"\"\"\n",
    "        \n",
    "        if not self.validate_properties():\n",
    "            raise ValueError(\"Table properties validation failed\")\n",
    "        \n",
    "        full_table_name = f\"{self.catalog}.{self.schema}.{table_name}\"\n",
    "        \n",
    "        # Add system properties\n",
    "        self.table_properties['created_by'] = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "        self.table_properties['created_date'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        print(f\"\\nCreating governed table: {full_table_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nTable Properties:\")\n",
    "        for key, value in self.table_properties.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        if df is not None:\n",
    "            # Create table from DataFrame\n",
    "            writer = df.write.format(\"delta\").mode(mode)\n",
    "            \n",
    "            # Add table properties\n",
    "            for key, value in self.table_properties.items():\n",
    "                writer = writer.option(f\"delta.property.{key}\", str(value))\n",
    "            \n",
    "            writer.saveAsTable(full_table_name)\n",
    "            print(f\"\\n✓ Table created successfully with {df.count()} rows\")\n",
    "        else:\n",
    "            # Create empty table with schema\n",
    "            empty_df = spark.createDataFrame([], schema)\n",
    "            writer = empty_df.write.format(\"delta\").mode(mode)\n",
    "            \n",
    "            for key, value in self.table_properties.items():\n",
    "                writer = writer.option(f\"delta.property.{key}\", str(value))\n",
    "            \n",
    "            writer.saveAsTable(full_table_name)\n",
    "            print(f\"\\n✓ Empty table created successfully\")\n",
    "        \n",
    "        return full_table_name\n",
    "    \n",
    "    def add_table_comment(self, table_name: str, comment: str):\n",
    "        \"\"\"Add descriptive comment to table\"\"\"\n",
    "        full_table_name = f\"{self.catalog}.{self.schema}.{table_name}\"\n",
    "        spark.sql(f\"COMMENT ON TABLE {full_table_name} IS '{comment}'\")\n",
    "        print(f\"✓ Comment added to {full_table_name}\")\n",
    "    \n",
    "    def add_column_comments(self, table_name: str, column_comments: dict):\n",
    "        \"\"\"Add comments to columns\"\"\"\n",
    "        full_table_name = f\"{self.catalog}.{self.schema}.{table_name}\"\n",
    "        \n",
    "        for column, comment in column_comments.items():\n",
    "            spark.sql(f\"ALTER TABLE {full_table_name} ALTER COLUMN {column} COMMENT '{comment}'\")\n",
    "            print(f\"✓ Comment added to column: {column}\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Governed Table Builder initialized\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"\"\"builder = GovernedTableBuilder('prod_catalog', 'silver')\n",
    "builder.set_properties(\n",
    "    owner='data_engineering_team',\n",
    "    domain='clinical',\n",
    "    pii_flag='true',\n",
    "    retention_days='2555',\n",
    "    data_classification='confidential',\n",
    "    quality_tier='silver',\n",
    "    update_frequency='daily',\n",
    "    source_system='emr_system',\n",
    "    business_owner='clinical.lead@example.com'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e74248-a402-40a6-a1dc-a9fe42e674e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Sample Governed Table"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Create a governed patient demographics table\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, BooleanType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define schema with standard naming conventions\n",
    "patient_schema = StructType([\n",
    "    StructField(\"patient_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", DateType(), True),\n",
    "    StructField(\"gender_code\", StringType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"created_timestamp\", TimestampType(), False),\n",
    "    StructField(\"updated_timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create sample data\n",
    "sample_data = [\n",
    "    (1, \"John\", \"Doe\", datetime(1980, 5, 15).date(), \"M\", True, datetime.now(), datetime.now()),\n",
    "    (2, \"Jane\", \"Smith\", datetime(1992, 8, 22).date(), \"F\", True, datetime.now(), datetime.now()),\n",
    "    (3, \"Robert\", \"Johnson\", datetime(1975, 3, 10).date(), \"M\", True, datetime.now(), datetime.now())\n",
    "]\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_data, patient_schema)\n",
    "\n",
    "print(\"Sample patient demographics data:\")\n",
    "display(sample_df)\n",
    "\n",
    "print(\"\\n✓ Sample data created with standardized schema\")\n",
    "print(f\"  - Columns follow snake_case naming\")\n",
    "print(f\"  - IDs use _id suffix\")\n",
    "print(f\"  - Dates use _date suffix\")\n",
    "print(f\"  - Timestamps use _timestamp suffix\")\n",
    "print(f\"  - Flags use is_ prefix and BOOLEAN type\")\n",
    "print(f\"  - Codes use _code suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba121709-4842-495c-a38c-c89e6fc4259b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Quality Gates & Data Validation\n",
    "\n",
    "### Quality Gate Levels\n",
    "\n",
    "**Bronze Layer (Raw Data):**\n",
    "* Schema validation (expected columns present)\n",
    "* Data type validation\n",
    "* Record count thresholds\n",
    "* Duplicate detection\n",
    "\n",
    "**Silver Layer (Cleaned Data):**\n",
    "* All Bronze checks\n",
    "* Null value constraints\n",
    "* Referential integrity\n",
    "* Business rule validation\n",
    "* Data freshness checks\n",
    "\n",
    "**Gold Layer (Curated Data):**\n",
    "* All Silver checks\n",
    "* Aggregation accuracy\n",
    "* Metric consistency\n",
    "* Historical trend validation\n",
    "\n",
    "### Quality Dimensions\n",
    "\n",
    "| Dimension | Description | Example Check |\n",
    "|-----------|-------------|---------------|\n",
    "| **Completeness** | Required fields populated | NULL count = 0 for NOT NULL columns |\n",
    "| **Accuracy** | Values within valid ranges | age BETWEEN 0 AND 120 |\n",
    "| **Consistency** | Data conforms to standards | date formats, code values |\n",
    "| **Timeliness** | Data is current | max(updated_date) within SLA |\n",
    "| **Uniqueness** | No duplicate records | DISTINCT count = total count |\n",
    "| **Validity** | Values match constraints | foreign keys exist |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b03dbb0b-1ded-4644-9cf6-7dd134f86951",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quality Gate Implementation"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive quality gate framework\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "class QualityGate:\n",
    "    \"\"\"Data quality validation framework\"\"\"\n",
    "    \n",
    "    def __init__(self, df: DataFrame, table_name: str, layer: str):\n",
    "        self.df = df\n",
    "        self.table_name = table_name\n",
    "        self.layer = layer\n",
    "        self.checks = []\n",
    "        self.results = []\n",
    "        self.passed = True\n",
    "    \n",
    "    def check_schema(self, expected_columns: list) -> 'QualityGate':\n",
    "        \"\"\"Validate expected columns are present\"\"\"\n",
    "        actual_columns = set(self.df.columns)\n",
    "        expected_columns_set = set(expected_columns)\n",
    "        \n",
    "        missing = expected_columns_set - actual_columns\n",
    "        extra = actual_columns - expected_columns_set\n",
    "        \n",
    "        passed = len(missing) == 0\n",
    "        self.passed = self.passed and passed\n",
    "        \n",
    "        self.results.append({\n",
    "            'check': 'Schema Validation',\n",
    "            'passed': passed,\n",
    "            'details': f\"Missing: {list(missing) if missing else 'None'}, Extra: {list(extra) if extra else 'None'}\",\n",
    "            'severity': 'CRITICAL'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def check_not_empty(self, min_rows: int = 1) -> 'QualityGate':\n",
    "        \"\"\"Validate DataFrame is not empty\"\"\"\n",
    "        count = self.df.count()\n",
    "        passed = count >= min_rows\n",
    "        self.passed = self.passed and passed\n",
    "        \n",
    "        self.results.append({\n",
    "            'check': 'Not Empty',\n",
    "            'passed': passed,\n",
    "            'details': f\"Row count: {count} (minimum: {min_rows})\",\n",
    "            'severity': 'CRITICAL'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def check_no_nulls(self, columns: list) -> 'QualityGate':\n",
    "        \"\"\"Validate specified columns have no null values\"\"\"\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                self.results.append({\n",
    "                    'check': f'No Nulls - {col}',\n",
    "                    'passed': False,\n",
    "                    'details': f\"Column '{col}' not found in DataFrame\",\n",
    "                    'severity': 'HIGH'\n",
    "                })\n",
    "                self.passed = False\n",
    "                continue\n",
    "            \n",
    "            null_count = self.df.filter(F.col(col).isNull()).count()\n",
    "            total_count = self.df.count()\n",
    "            passed = null_count == 0\n",
    "            self.passed = self.passed and passed\n",
    "            \n",
    "            self.results.append({\n",
    "                'check': f'No Nulls - {col}',\n",
    "                'passed': passed,\n",
    "                'details': f\"Null count: {null_count}/{total_count} ({null_count/total_count*100:.2f}%)\",\n",
    "                'severity': 'HIGH'\n",
    "            })\n",
    "        return self\n",
    "    \n",
    "    def check_unique(self, columns: list) -> 'QualityGate':\n",
    "        \"\"\"Validate uniqueness of specified columns\"\"\"\n",
    "        total_count = self.df.count()\n",
    "        distinct_count = self.df.select(columns).distinct().count()\n",
    "        duplicate_count = total_count - distinct_count\n",
    "        \n",
    "        passed = duplicate_count == 0\n",
    "        self.passed = self.passed and passed\n",
    "        \n",
    "        self.results.append({\n",
    "            'check': f'Uniqueness - {columns}',\n",
    "            'passed': passed,\n",
    "            'details': f\"Duplicates: {duplicate_count}/{total_count} ({duplicate_count/total_count*100:.2f}%)\",\n",
    "            'severity': 'HIGH'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def check_value_range(self, column: str, min_val=None, max_val=None) -> 'QualityGate':\n",
    "        \"\"\"Validate values are within specified range\"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            self.results.append({\n",
    "                'check': f'Value Range - {column}',\n",
    "                'passed': False,\n",
    "                'details': f\"Column '{column}' not found\",\n",
    "                'severity': 'MEDIUM'\n",
    "            })\n",
    "            self.passed = False\n",
    "            return self\n",
    "        \n",
    "        condition = F.lit(True)\n",
    "        if min_val is not None:\n",
    "            condition = condition & (F.col(column) >= min_val)\n",
    "        if max_val is not None:\n",
    "            condition = condition & (F.col(column) <= max_val)\n",
    "        \n",
    "        invalid_count = self.df.filter(~condition).count()\n",
    "        total_count = self.df.count()\n",
    "        passed = invalid_count == 0\n",
    "        self.passed = self.passed and passed\n",
    "        \n",
    "        range_str = f\"[{min_val if min_val is not None else '-∞'}, {max_val if max_val is not None else '∞'}]\"\n",
    "        self.results.append({\n",
    "            'check': f'Value Range - {column}',\n",
    "            'passed': passed,\n",
    "            'details': f\"Out of range {range_str}: {invalid_count}/{total_count} ({invalid_count/total_count*100:.2f}%)\",\n",
    "            'severity': 'MEDIUM'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def check_freshness(self, timestamp_column: str, max_age_hours: int = 24) -> 'QualityGate':\n",
    "        \"\"\"Validate data freshness\"\"\"\n",
    "        if timestamp_column not in self.df.columns:\n",
    "            self.results.append({\n",
    "                'check': 'Data Freshness',\n",
    "                'passed': False,\n",
    "                'details': f\"Column '{timestamp_column}' not found\",\n",
    "                'severity': 'MEDIUM'\n",
    "            })\n",
    "            self.passed = False\n",
    "            return self\n",
    "        \n",
    "        max_timestamp = self.df.agg(F.max(timestamp_column)).collect()[0][0]\n",
    "        \n",
    "        if max_timestamp is None:\n",
    "            passed = False\n",
    "            age_hours = None\n",
    "        else:\n",
    "            age = datetime.now() - max_timestamp\n",
    "            age_hours = age.total_seconds() / 3600\n",
    "            passed = age_hours <= max_age_hours\n",
    "        \n",
    "        self.passed = self.passed and passed\n",
    "        \n",
    "        self.results.append({\n",
    "            'check': 'Data Freshness',\n",
    "            'passed': passed,\n",
    "            'details': f\"Latest record: {max_timestamp}, Age: {age_hours:.1f}h (max: {max_age_hours}h)\" if age_hours else \"No timestamp data\",\n",
    "            'severity': 'MEDIUM'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def check_referential_integrity(self, column: str, reference_df: DataFrame, reference_column: str) -> 'QualityGate':\n",
    "        \"\"\"Validate foreign key relationships\"\"\"\n",
    "        valid_values = reference_df.select(reference_column).distinct()\n",
    "        \n",
    "        invalid_count = self.df.join(\n",
    "            valid_values,\n",
    "            self.df[column] == valid_values[reference_column],\n",
    "            \"left_anti\"\n",
    "        ).count()\n",
    "        \n",
    "        total_count = self.df.count()\n",
    "        passed = invalid_count == 0\n",
    "        self.passed = self.passed and passed\n",
    "        \n",
    "        self.results.append({\n",
    "            'check': f'Referential Integrity - {column}',\n",
    "            'passed': passed,\n",
    "            'details': f\"Invalid references: {invalid_count}/{total_count} ({invalid_count/total_count*100:.2f}%)\",\n",
    "            'severity': 'HIGH'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def execute(self) -> dict:\n",
    "        \"\"\"Execute all checks and return results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"QUALITY GATE REPORT: {self.table_name} ({self.layer.upper()} Layer)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Total Checks: {len(self.results)}\")\n",
    "        \n",
    "        passed_count = sum(1 for r in self.results if r['passed'])\n",
    "        failed_count = len(self.results) - passed_count\n",
    "        \n",
    "        print(f\"Passed: {passed_count} | Failed: {failed_count}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in self.results:\n",
    "            status = \"✓ PASS\" if result['passed'] else \"✗ FAIL\"\n",
    "            severity = result['severity']\n",
    "            print(f\"{status} [{severity:8}] {result['check']}\")\n",
    "            print(f\"           {result['details']}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if self.passed:\n",
    "            print(\"\\n✓ ALL QUALITY GATES PASSED - Data approved for promotion\")\n",
    "        else:\n",
    "            print(\"\\n✗ QUALITY GATES FAILED - Data requires remediation\")\n",
    "        \n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            'table_name': self.table_name,\n",
    "            'layer': self.layer,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'passed': self.passed,\n",
    "            'total_checks': len(self.results),\n",
    "            'passed_checks': passed_count,\n",
    "            'failed_checks': failed_count,\n",
    "            'results': self.results\n",
    "        }\n",
    "\n",
    "print(\"✓ Quality Gate framework loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a911938e-7b16-4094-acf9-5c320e1f7ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Standards Enforcement & Compliance\n",
    "\n",
    "### Automated Enforcement Mechanisms\n",
    "\n",
    "**1. Pre-Commit Validation**\n",
    "* Schema validation before table creation\n",
    "* Naming convention checks\n",
    "* Required metadata validation\n",
    "\n",
    "**2. Continuous Monitoring**\n",
    "* Daily quality gate execution\n",
    "* Automated alerts on failures\n",
    "* Trend analysis and reporting\n",
    "\n",
    "**3. Access Auditing**\n",
    "* Regular permission reviews\n",
    "* Access log analysis\n",
    "* Anomaly detection\n",
    "\n",
    "### Priority Dataset Classification\n",
    "\n",
    "**Tier 1 - Critical:**\n",
    "* Patient demographics\n",
    "* Clinical encounters\n",
    "* Medication orders\n",
    "* Lab results\n",
    "* **SLA:** 99.9% quality score, < 1 hour data latency\n",
    "\n",
    "**Tier 2 - Important:**\n",
    "* Operational metrics\n",
    "* Financial transactions\n",
    "* Inventory data\n",
    "* **SLA:** 99% quality score, < 4 hour data latency\n",
    "\n",
    "**Tier 3 - Standard:**\n",
    "* Reference data\n",
    "* Lookup tables\n",
    "* Historical archives\n",
    "* **SLA:** 95% quality score, < 24 hour data latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cd55e8d-a311-401a-a4ea-353020c692c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standards Validator"
    }
   },
   "outputs": [],
   "source": [
    "# Automated standards enforcement\n",
    "\n",
    "import re\n",
    "\n",
    "class StandardsValidator:\n",
    "    \"\"\"Validate compliance with data governance standards\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.violations = []\n",
    "    \n",
    "    def validate_table_name(self, table_name: str) -> bool:\n",
    "        \"\"\"Validate table naming convention\"\"\"\n",
    "        # Pattern: {domain}_{entity}_{type}\n",
    "        pattern = r'^[a-z]+_[a-z_]+_(dim|fact|raw|agg|bridge|ref)$'\n",
    "        \n",
    "        if not re.match(pattern, table_name):\n",
    "            self.violations.append({\n",
    "                'rule': 'Table Naming Convention',\n",
    "                'severity': 'HIGH',\n",
    "                'message': f\"Table name '{table_name}' does not follow pattern: {{domain}}_{{entity}}_{{type}}\",\n",
    "                'example': 'patient_demographics_dim, clinical_encounters_fact'\n",
    "            })\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def validate_column_names(self, columns: list) -> bool:\n",
    "        \"\"\"Validate column naming conventions\"\"\"\n",
    "        valid = True\n",
    "        \n",
    "        for col in columns:\n",
    "            # Check snake_case\n",
    "            if not re.match(r'^[a-z][a-z0-9_]*$', col):\n",
    "                self.violations.append({\n",
    "                    'rule': 'Column Naming Convention',\n",
    "                    'severity': 'MEDIUM',\n",
    "                    'message': f\"Column '{col}' must use snake_case (lowercase with underscores)\",\n",
    "                    'example': 'patient_id, first_name, created_timestamp'\n",
    "                })\n",
    "                valid = False\n",
    "            \n",
    "            # Check for reserved suffixes\n",
    "            if col.endswith('_id') and not col.replace('_id', '').isidentifier():\n",
    "                self.violations.append({\n",
    "                    'rule': 'ID Column Standard',\n",
    "                    'severity': 'LOW',\n",
    "                    'message': f\"ID column '{col}' should follow pattern: {{entity}}_id\",\n",
    "                    'example': 'patient_id, encounter_id'\n",
    "                })\n",
    "                valid = False\n",
    "        \n",
    "        return valid\n",
    "    \n",
    "    def validate_table_properties(self, properties: dict) -> bool:\n",
    "        \"\"\"Validate required table properties\"\"\"\n",
    "        required = [\n",
    "            'owner', 'domain', 'pii_flag', 'retention_days',\n",
    "            'data_classification', 'quality_tier', 'update_frequency',\n",
    "            'source_system', 'business_owner'\n",
    "        ]\n",
    "        \n",
    "        missing = [prop for prop in required if prop not in properties]\n",
    "        \n",
    "        if missing:\n",
    "            self.violations.append({\n",
    "                'rule': 'Required Table Properties',\n",
    "                'severity': 'CRITICAL',\n",
    "                'message': f\"Missing required properties: {', '.join(missing)}\",\n",
    "                'example': 'All tables must have owner, domain, pii_flag, etc.'\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "        # Validate property values\n",
    "        valid_domains = ['clinical', 'operational', 'analytics', 'reference']\n",
    "        if properties.get('domain') not in valid_domains:\n",
    "            self.violations.append({\n",
    "                'rule': 'Domain Classification',\n",
    "                'severity': 'HIGH',\n",
    "                'message': f\"Invalid domain: {properties.get('domain')}. Must be one of: {valid_domains}\",\n",
    "                'example': 'domain=clinical'\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "        valid_classifications = ['public', 'internal', 'confidential', 'restricted']\n",
    "        if properties.get('data_classification') not in valid_classifications:\n",
    "            self.violations.append({\n",
    "                'rule': 'Data Classification',\n",
    "                'severity': 'CRITICAL',\n",
    "                'message': f\"Invalid classification: {properties.get('data_classification')}. Must be one of: {valid_classifications}\",\n",
    "                'example': 'data_classification=confidential'\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def validate_data_types(self, schema: StructType) -> bool:\n",
    "        \"\"\"Validate data type standards\"\"\"\n",
    "        valid = True\n",
    "        \n",
    "        for field in schema.fields:\n",
    "            # Check ID columns use appropriate types\n",
    "            if field.name.endswith('_id'):\n",
    "                if not isinstance(field.dataType, (IntegerType, StringType)):\n",
    "                    self.violations.append({\n",
    "                        'rule': 'ID Column Data Type',\n",
    "                        'severity': 'MEDIUM',\n",
    "                        'message': f\"Column '{field.name}' should use BIGINT or STRING type\",\n",
    "                        'example': 'patient_id BIGINT'\n",
    "                    })\n",
    "                    valid = False\n",
    "            \n",
    "            # Check date columns use DATE type\n",
    "            if field.name.endswith('_date'):\n",
    "                if not isinstance(field.dataType, DateType):\n",
    "                    self.violations.append({\n",
    "                        'rule': 'Date Column Data Type',\n",
    "                        'severity': 'MEDIUM',\n",
    "                        'message': f\"Column '{field.name}' should use DATE type\",\n",
    "                        'example': 'admission_date DATE'\n",
    "                    })\n",
    "                    valid = False\n",
    "            \n",
    "            # Check timestamp columns use TIMESTAMP type\n",
    "            if field.name.endswith('_timestamp'):\n",
    "                if not isinstance(field.dataType, TimestampType):\n",
    "                    self.violations.append({\n",
    "                        'rule': 'Timestamp Column Data Type',\n",
    "                        'severity': 'MEDIUM',\n",
    "                        'message': f\"Column '{field.name}' should use TIMESTAMP type\",\n",
    "                        'example': 'created_timestamp TIMESTAMP'\n",
    "                    })\n",
    "                    valid = False\n",
    "            \n",
    "            # Check flag columns use BOOLEAN type\n",
    "            if field.name.startswith('is_') or field.name.endswith('_flag'):\n",
    "                if not isinstance(field.dataType, BooleanType):\n",
    "                    self.violations.append({\n",
    "                        'rule': 'Boolean Column Data Type',\n",
    "                        'severity': 'LOW',\n",
    "                        'message': f\"Column '{field.name}' should use BOOLEAN type\",\n",
    "                        'example': 'is_active BOOLEAN'\n",
    "                    })\n",
    "                    valid = False\n",
    "        \n",
    "        return valid\n",
    "    \n",
    "    def report(self) -> bool:\n",
    "        \"\"\"Generate compliance report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STANDARDS COMPLIANCE REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if not self.violations:\n",
    "            print(\"\\n✓ ALL STANDARDS CHECKS PASSED\")\n",
    "            print(\"  No violations detected\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"\\n✗ FOUND {len(self.violations)} VIOLATION(S)\\n\")\n",
    "        \n",
    "        # Group by severity\n",
    "        critical = [v for v in self.violations if v['severity'] == 'CRITICAL']\n",
    "        high = [v for v in self.violations if v['severity'] == 'HIGH']\n",
    "        medium = [v for v in self.violations if v['severity'] == 'MEDIUM']\n",
    "        low = [v for v in self.violations if v['severity'] == 'LOW']\n",
    "        \n",
    "        for severity, violations in [('CRITICAL', critical), ('HIGH', high), ('MEDIUM', medium), ('LOW', low)]:\n",
    "            if violations:\n",
    "                print(f\"\\n{severity} SEVERITY ({len(violations)}):\")\n",
    "                print(\"-\" * 80)\n",
    "                for v in violations:\n",
    "                    print(f\"\\n  Rule: {v['rule']}\")\n",
    "                    print(f\"  Issue: {v['message']}\")\n",
    "                    print(f\"  Example: {v['example']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\\n✗ STANDARDS VALIDATION FAILED - Remediation required\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "print(\"✓ Standards Validator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02ba9f6e-2625-491a-8297-b7d47556c376",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standards Validation Example"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Validate standards compliance\n",
    "\n",
    "validator = StandardsValidator()\n",
    "\n",
    "# Validate table name\n",
    "validator.validate_table_name(\"patient_demographics_dim\")\n",
    "\n",
    "# Validate column names\n",
    "validator.validate_column_names(sample_df.columns)\n",
    "\n",
    "# Validate table properties\n",
    "properties = {\n",
    "    'owner': 'data_engineering_team',\n",
    "    'domain': 'clinical',\n",
    "    'pii_flag': 'true',\n",
    "    'retention_days': '2555',\n",
    "    'data_classification': 'confidential',\n",
    "    'quality_tier': 'silver',\n",
    "    'update_frequency': 'daily',\n",
    "    'source_system': 'emr_system',\n",
    "    'business_owner': 'clinical.lead@example.com'\n",
    "}\n",
    "validator.validate_table_properties(properties)\n",
    "\n",
    "# Validate data types\n",
    "validator.validate_data_types(patient_schema)\n",
    "\n",
    "# Generate compliance report\n",
    "compliant = validator.report()\n",
    "\n",
    "if compliant:\n",
    "    print(\"✓ Table meets all governance standards\")\n",
    "else:\n",
    "    print(\"✗ Table requires updates to meet standards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed251891-047c-4a9b-9afa-ff3b8aa1cd5b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Monitoring & Compliance"
    }
   },
   "source": [
    "## 6. Continuous Monitoring & Compliance\n",
    "\n",
    "### Automated Monitoring Framework\n",
    "\n",
    "**Daily Checks:**\n",
    "* Quality gate execution on all Tier 1 datasets\n",
    "* Schema drift detection\n",
    "* Data volume anomaly detection\n",
    "* Access pattern analysis\n",
    "\n",
    "**Weekly Reviews:**\n",
    "* Permission audit reports\n",
    "* Data lineage validation\n",
    "* Metadata completeness checks\n",
    "* SLA compliance reporting\n",
    "\n",
    "**Monthly Governance Reviews:**\n",
    "* Catalog-wide compliance assessment\n",
    "* Unused table identification\n",
    "* Cost optimization opportunities\n",
    "* Policy effectiveness evaluation\n",
    "\n",
    "### Compliance Metrics\n",
    "\n",
    "| Metric | Target | Measurement |\n",
    "|--------|--------|-------------|\n",
    "| **Metadata Completeness** | 100% | Tables with all required properties |\n",
    "| **Quality Gate Pass Rate** | >95% | Successful validations / Total runs |\n",
    "| **Access Compliance** | 100% | Proper RBAC implementation |\n",
    "| **Data Freshness** | >98% | Tables meeting SLA thresholds |\n",
    "| **Schema Standardization** | >90% | Tables following naming conventions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Auditing Queries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Audit Query 1: Find tables without required metadata\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  table_type,\n",
    "  comment\n",
    "FROM prod_catalog.information_schema.tables\n",
    "WHERE table_schema IN ('bronze', 'silver', 'gold')\n",
    "  AND (comment IS NULL OR comment = '')\n",
    "ORDER BY table_schema, table_name;\n",
    "\n",
    "-- Audit Query 2: Identify tables with missing ownership\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  table_owner\n",
    "FROM prod_catalog.information_schema.tables\n",
    "WHERE table_schema IN ('bronze', 'silver', 'gold')\n",
    "  AND (table_owner IS NULL OR table_owner = '')\n",
    "ORDER BY table_schema, table_name;\n",
    "\n",
    "-- Audit Query 3: Check for non-standard table names\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  CASE \n",
    "    WHEN table_name NOT REGEXP '^[a-z]+_[a-z_]+_(dim|fact|raw|agg|bridge|ref)$' \n",
    "    THEN 'Non-standard naming'\n",
    "    ELSE 'Compliant'\n",
    "  END AS naming_status\n",
    "FROM prod_catalog.information_schema.tables\n",
    "WHERE table_schema IN ('bronze', 'silver', 'gold')\n",
    "  AND table_name NOT REGEXP '^[a-z]+_[a-z_]+_(dim|fact|raw|agg|bridge|ref)$'\n",
    "ORDER BY table_schema, table_name;\n",
    "\n",
    "-- Audit Query 4: Find tables not accessed in last 30 days\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  DATEDIFF(CURRENT_DATE(), last_altered) AS days_since_modified\n",
    "FROM prod_catalog.information_schema.tables\n",
    "WHERE table_schema IN ('bronze', 'silver', 'gold')\n",
    "  AND DATEDIFF(CURRENT_DATE(), last_altered) > 30\n",
    "ORDER BY days_since_modified DESC;\n",
    "\n",
    "-- Audit Query 5: Review current permissions by principal\n",
    "SHOW GRANTS ON CATALOG prod_catalog;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Collect governance metrics for dashboard reporting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67c214d-cb62-4468-bb00-851e057c4457",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Governance Dashboard Metrics"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class GovernanceMetrics:\n",
    "    \"\"\"Collect and report governance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, catalog: str):\n",
    "        self.catalog = catalog\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def get_table_count_by_layer(self):\n",
    "        \"\"\"Count tables in each layer\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            table_schema AS layer,\n",
    "            COUNT(*) AS table_count\n",
    "        FROM {self.catalog}.information_schema.tables\n",
    "        WHERE table_schema IN ('bronze', 'silver', 'gold', 'sandbox')\n",
    "        GROUP BY table_schema\n",
    "        ORDER BY table_schema\n",
    "        \"\"\"\n",
    "        \n",
    "        df = spark.sql(query)\n",
    "        self.metrics['table_count_by_layer'] = df.toPandas().to_dict('records')\n",
    "        return df\n",
    "    \n",
    "    def get_metadata_completeness(self):\n",
    "        \"\"\"Calculate metadata completeness percentage\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            table_schema AS layer,\n",
    "            COUNT(*) AS total_tables,\n",
    "            SUM(CASE WHEN comment IS NOT NULL AND comment != '' THEN 1 ELSE 0 END) AS tables_with_comments,\n",
    "            ROUND(SUM(CASE WHEN comment IS NOT NULL AND comment != '' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS completeness_pct\n",
    "        FROM {self.catalog}.information_schema.tables\n",
    "        WHERE table_schema IN ('bronze', 'silver', 'gold')\n",
    "        GROUP BY table_schema\n",
    "        ORDER BY table_schema\n",
    "        \"\"\"\n",
    "        \n",
    "        df = spark.sql(query)\n",
    "        self.metrics['metadata_completeness'] = df.toPandas().to_dict('records')\n",
    "        return df\n",
    "    \n",
    "    def get_column_count_stats(self):\n",
    "        \"\"\"Get column count statistics by layer\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            table_schema AS layer,\n",
    "            COUNT(DISTINCT table_name) AS table_count,\n",
    "            COUNT(*) AS total_columns,\n",
    "            ROUND(AVG(column_count), 2) AS avg_columns_per_table\n",
    "        FROM (\n",
    "            SELECT \n",
    "                table_schema,\n",
    "                table_name,\n",
    "                COUNT(*) AS column_count\n",
    "            FROM {self.catalog}.information_schema.columns\n",
    "            WHERE table_schema IN ('bronze', 'silver', 'gold')\n",
    "            GROUP BY table_schema, table_name\n",
    "        )\n",
    "        GROUP BY table_schema\n",
    "        ORDER BY table_schema\n",
    "        \"\"\"\n",
    "        \n",
    "        df = spark.sql(query)\n",
    "        self.metrics['column_stats'] = df.toPandas().to_dict('records')\n",
    "        return df\n",
    "    \n",
    "    def get_data_classification_summary(self):\n",
    "        \"\"\"Summarize tables by data classification (from table properties)\"\"\"\n",
    "        # Note: This would query table properties if stored\n",
    "        # For now, return a placeholder structure\n",
    "        classification_data = [\n",
    "            {'classification': 'confidential', 'table_count': 0, 'percentage': 0.0},\n",
    "            {'classification': 'internal', 'table_count': 0, 'percentage': 0.0},\n",
    "            {'classification': 'public', 'table_count': 0, 'percentage': 0.0}\n",
    "        ]\n",
    "        \n",
    "        self.metrics['data_classification'] = classification_data\n",
    "        print(\"Note: Data classification metrics require table properties to be set\")\n",
    "        return spark.createDataFrame(classification_data)\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive governance report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"GOVERNANCE METRICS REPORT: {self.catalog}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\n1. TABLE COUNT BY LAYER\")\n",
    "        print(\"-\" * 80)\n",
    "        df1 = self.get_table_count_by_layer()\n",
    "        display(df1)\n",
    "        \n",
    "        print(\"\\n2. METADATA COMPLETENESS\")\n",
    "        print(\"-\" * 80)\n",
    "        df2 = self.get_metadata_completeness()\n",
    "        display(df2)\n",
    "        \n",
    "        print(\"\\n3. COLUMN STATISTICS\")\n",
    "        print(\"-\" * 80)\n",
    "        df3 = self.get_column_count_stats()\n",
    "        display(df3)\n",
    "        \n",
    "        print(\"\\n4. DATA CLASSIFICATION SUMMARY\")\n",
    "        print(\"-\" * 80)\n",
    "        df4 = self.get_data_classification_summary()\n",
    "        display(df4)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"END OF REPORT\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "\n",
    "# Example usage\n",
    "print(\"Governance Metrics collector initialized\")\n",
    "print(\"\\nUsage: metrics = GovernanceMetrics('prod_catalog')\")\n",
    "print(\"       metrics.generate_report()\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Governance & Standardization Framework - Unity Cat ...",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
